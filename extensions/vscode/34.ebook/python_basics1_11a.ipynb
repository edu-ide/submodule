{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ 2ê¶Œ 1ì¥: ì›¹ ìŠ¤í¬ë˜í•‘ (í¬ë¡¤ë§)\n",
    "\n",
    "## ğŸ“Œ ëª©ì°¨\n",
    "11.1 ì›¹ ìŠ¤í¬ë˜í•‘ ê°œë… ë° í™œìš©  \n",
    "11.2 requestsì™€ BeautifulSoup ì‚¬ìš©ë²•  \n",
    "11.3 Seleniumì„ í™œìš©í•œ ë™ì  ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§  \n",
    "11.4 í¬ë¡¤ë§í•œ ë°ì´í„° ì €ì¥ ë° ë¶„ì„  \n",
    "\n",
    "## 11.1 ì›¹ ìŠ¤í¬ë˜í•‘ ê°œë… ë° í™œìš©\n",
    "\n",
    "### âœ… 11.1.1 ì›¹ ìŠ¤í¬ë˜í•‘ì˜ ì£¼ìš” í™œìš© ë¶„ì•¼\n",
    "1. **ë°ì´í„° ìˆ˜ì§‘**\n",
    "   - ì‹œì¥ ì¡°ì‚¬\n",
    "   - ê°€ê²© ë¹„êµ\n",
    "   - íŠ¸ë Œë“œ ë¶„ì„\n",
    "\n",
    "2. **ìë™í™”**\n",
    "   - ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§\n",
    "   - ì¬ê³  ê´€ë¦¬\n",
    "   - ê²½ìŸì‚¬ ë¶„ì„\n",
    "\n",
    "3. **ì—°êµ¬ ë° ë¶„ì„**\n",
    "   - ì†Œì…œ ë¯¸ë””ì–´ ë¶„ì„\n",
    "   - ì—¬ë¡  ì¡°ì‚¬\n",
    "   - í•™ìˆ  ì—°êµ¬\n",
    "\n",
    "### âœ… 11.1.2 ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ ì£¼ì˜ì‚¬í•­\n",
    "1. **ë²•ì  ê³ ë ¤ì‚¬í•­**\n",
    "   - robots.txt í™•ì¸\n",
    "   - ì´ìš©ì•½ê´€ ì¤€ìˆ˜\n",
    "   - ê°œì¸ì •ë³´ ë³´í˜¸\n",
    "\n",
    "2. **ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­**\n",
    "   - ìš”ì²­ ê°„ê²© ì¡°ì ˆ\n",
    "   - ì„œë²„ ë¶€í•˜ ê³ ë ¤\n",
    "   - ì—ëŸ¬ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4870ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def get_page(self, url):\n",
    "        \"\"\"ì›¹ í˜ì´ì§€ ìš”ì²­ ë° ì—ëŸ¬ ì²˜ë¦¬\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_html(self, html):\n",
    "        \"\"\"HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "        if html:\n",
    "            return BeautifulSoup(html, 'html.parser')\n",
    "        return None\n",
    "    \n",
    "    def delay_request(self):\n",
    "        \"\"\"ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€\"\"\"\n",
    "        time.sleep(random.uniform(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05002b1b",
   "metadata": {},
   "source": [
    "## 11.2 ì›¹ ìŠ¤í¬ë˜í•‘ ê³ ê¸‰ ê¸°ë²•\n",
    "\n",
    "### âœ… 11.2.1 ê³ ê¸‰ BeautifulSoup ì‚¬ìš©ë²•\n",
    "1. **CSS ì„ íƒì í™œìš©**\n",
    "2. **ì •ê·œí‘œí˜„ì‹ ê²°í•©**\n",
    "3. **ê³„ì¸µ êµ¬ì¡° íƒìƒ‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class AdvancedScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def get_elements_by_css(self, html, selector):\n",
    "        \"\"\"CSS ì„ íƒìë¡œ ìš”ì†Œ ì°¾ê¸°\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup.select(selector)\n",
    "    \n",
    "    def find_by_regex(self, html, pattern):\n",
    "        \"\"\"ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë‚´ìš© ì°¾ê¸°\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup.find(text=re.compile(pattern))\n",
    "    \n",
    "    def navigate_structure(self, element):\n",
    "        \"\"\"ìš”ì†Œì˜ ê³„ì¸µ êµ¬ì¡° íƒìƒ‰\"\"\"\n",
    "        parent = element.parent\n",
    "        siblings = element.find_next_siblings()\n",
    "        children = element.find_all(recursive=False)\n",
    "        return parent, siblings, children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dd3b0",
   "metadata": {},
   "source": [
    "### âœ… 11.2.2 ë™ì  í˜ì´ì§€ ì²˜ë¦¬\n",
    "1. **JavaScript ë Œë”ë§ ëŒ€ì‘**\n",
    "2. **AJAX ìš”ì²­ ì²˜ë¦¬**\n",
    "3. **ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class DynamicScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "    \n",
    "    def scroll_to_bottom(self):\n",
    "        \"\"\"ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "    \n",
    "    def wait_for_element(self, selector):\n",
    "        \"\"\"ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°\"\"\"\n",
    "        return self.wait.until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "    \n",
    "    def handle_ajax(self, url):\n",
    "        \"\"\"AJAX ìš”ì²­ ì²˜ë¦¬\"\"\"\n",
    "        self.driver.get(url)\n",
    "        self.wait_for_element('#content')\n",
    "        return self.driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db5f1f",
   "metadata": {},
   "source": [
    "### âœ… 11.2.3 BeautifulSoup ê³ ê¸‰ ì‚¬ìš©ë²•\n",
    "1. **CSS ì„ íƒì í™œìš©**\n",
    "2. **ì†ì„± ê¸°ë°˜ ê²€ìƒ‰**\n",
    "3. **ì •ê·œí‘œí˜„ì‹ í™œìš©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ca048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup ê³ ê¸‰ ì„ íƒì ì˜ˆì œ\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def advanced_soup_usage(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # CSS ì„ íƒì ì‚¬ìš©\n",
    "    articles = soup.select('div.article-content p')\n",
    "    main_content = soup.select('#main-content')\n",
    "    \n",
    "    # ì†ì„±ìœ¼ë¡œ ì°¾ê¸°\n",
    "    external_links = soup.find_all('a', attrs={'target': '_blank'})\n",
    "    \n",
    "    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ê¸°\n",
    "    post_divs = soup.find_all('div', class_=re.compile('^post-'))\n",
    "    \n",
    "    return articles, main_content, external_links, post_divs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b2a11",
   "metadata": {},
   "source": [
    "### âœ… 11.2.4 ì—ëŸ¬ ì²˜ë¦¬ì™€ ì˜ˆì™¸ ìƒí™©\n",
    "1. **ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬**\n",
    "2. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**\n",
    "3. **ì¬ì‹œë„ ë¡œì§**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f01691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import RequestException\n",
    "\n",
    "def safe_get_page(url, retries=3):\n",
    "    \"\"\"ì•ˆì „í•œ ì›¹ í˜ì´ì§€ ìš”ì²­\"\"\"\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except RequestException as e:\n",
    "            print(f\"ì‹œë„ {i+1}/{retries} ì‹¤íŒ¨: {e}\")\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep(2 ** i)  # ì§€ìˆ˜ ë°±ì˜¤í”„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0090c6b",
   "metadata": {},
   "source": [
    "## 11.3 ë°ì´í„° ì €ì¥ ë° ë¶„ì„\n",
    "\n",
    "### âœ… 11.3.1 ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\n",
    "1. **SQLite í™œìš©**\n",
    "2. **MongoDB í™œìš©**\n",
    "3. **ë°ì´í„° ì •ê·œí™”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f53250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class DataStorage:\n",
    "    def __init__(self):\n",
    "        self.sqlite_conn = sqlite3.connect('scraping.db')\n",
    "        self.mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "    \n",
    "    def save_to_sqlite(self, data):\n",
    "        \"\"\"SQLiteì— ë°ì´í„° ì €ì¥\"\"\"\n",
    "        cursor = self.sqlite_conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles\n",
    "            (title TEXT, url TEXT, content TEXT)\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO articles VALUES (?, ?, ?)\",\n",
    "            [(d['title'], d['url'], d['content']) for d in data]\n",
    "        )\n",
    "        self.sqlite_conn.commit()\n",
    "    \n",
    "    def save_to_mongodb(self, data):\n",
    "        \"\"\"MongoDBì— ë°ì´í„° ì €ì¥\"\"\"\n",
    "        db = self.mongo_client.scraping_db\n",
    "        db.articles.insert_many(data)\n",
    "    \n",
    "    def close(self):\n",
    "        self.sqlite_conn.close()\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc76f5",
   "metadata": {},
   "source": [
    "### âœ… 11.3.2 ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”\n",
    "1. **pandas í™œìš©**\n",
    "2. **matplotlib ì‹œê°í™”**\n",
    "3. **ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "    \n",
    "    def basic_analysis(self):\n",
    "        \"\"\"ê¸°ë³¸ í†µê³„ ë¶„ì„\"\"\"\n",
    "        return {\n",
    "            'total_articles': len(self.df),\n",
    "            'unique_sources': self.df['source'].nunique(),\n",
    "            'date_range': (self.df['date'].min(), self.df['date'].max())\n",
    "        }\n",
    "    \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"ë°ì´í„° ì‹œê°í™”\"\"\"\n",
    "        # ì‹œê°„ë³„ ê²Œì‹œë¬¼ ìˆ˜ ê·¸ë˜í”„\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.df['date'].value_counts().sort_index().plot()\n",
    "        plt.title('Posts Over Time')\n",
    "        plt.savefig('posts_timeline.png')\n",
    "        \n",
    "        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "        text = ' '.join(self.df['content'])\n",
    "        wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.savefig('wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520ead3",
   "metadata": {},
   "source": [
    "## 11.4 ë°ì´í„° ì €ì¥ ë° ë¶„ì„\n",
    "\n",
    "### âœ… 11.4.1 ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥\n",
    "1. **SQLite í™œìš©**\n",
    "2. **MongoDB í™œìš©**\n",
    "3. **ë°ì´í„° ì •ê·œí™”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class DataStorage:\n",
    "    def __init__(self):\n",
    "        self.sqlite_conn = sqlite3.connect('scraping.db')\n",
    "        self.mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "    \n",
    "    def save_to_sqlite(self, data):\n",
    "        \"\"\"SQLiteì— ë°ì´í„° ì €ì¥\"\"\"\n",
    "        cursor = self.sqlite_conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles\n",
    "            (title TEXT, url TEXT, content TEXT)\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO articles VALUES (?, ?, ?)\",\n",
    "            [(d['title'], d['url'], d['content']) for d in data]\n",
    "        )\n",
    "        self.sqlite_conn.commit()\n",
    "    \n",
    "    def save_to_mongodb(self, data):\n",
    "        \"\"\"MongoDBì— ë°ì´í„° ì €ì¥\"\"\"\n",
    "        db = self.mongo_client.scraping_db\n",
    "        db.articles.insert_many(data)\n",
    "    \n",
    "    def close(self):\n",
    "        self.sqlite_conn.close()\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6e60c",
   "metadata": {},
   "source": [
    "### âœ… 11.4.2 ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”\n",
    "1. **pandas í™œìš©**\n",
    "2. **matplotlib ì‹œê°í™”**\n",
    "3. **ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "    \n",
    "    def basic_analysis(self):\n",
    "        \"\"\"ê¸°ë³¸ í†µê³„ ë¶„ì„\"\"\"\n",
    "        return {\n",
    "            'total_articles': len(self.df),\n",
    "            'unique_sources': self.df['source'].nunique(),\n",
    "            'date_range': (self.df['date'].min(), self.df['date'].max())\n",
    "        }\n",
    "    \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"ë°ì´í„° ì‹œê°í™”\"\"\"\n",
    "        # ì‹œê°„ë³„ ê²Œì‹œë¬¼ ìˆ˜ ê·¸ë˜í”„\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.df['date'].value_counts().sort_index().plot()\n",
    "        plt.title('Posts Over Time')\n",
    "        plt.savefig('posts_timeline.png')\n",
    "        \n",
    "        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "        text = ' '.join(self.df['content'])\n",
    "        wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.savefig('wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832978a",
   "metadata": {},
   "source": [
    "### âœ… 11.4.3 í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "1. **í…ìŠ¤íŠ¸ ì •ì œ**\n",
    "2. **ë¶ˆìš©ì–´ ì œê±°**\n",
    "3. **í˜•íƒœì†Œ ë¶„ì„**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.okt = Okt()\n",
    "        self.stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ']\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "        # HTML íƒœê·¸ ì œê±°\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"í˜•íƒœì†Œ ë¶„ì„\"\"\"\n",
    "        # í…ìŠ¤íŠ¸ ì •ì œ\n",
    "        text = self.clean_text(text)\n",
    "        # í˜•íƒœì†Œ ë¶„ì„\n",
    "        tokens = self.okt.morphs(text)\n",
    "        # ë¶ˆìš©ì–´ ì œê±°\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c6e26",
   "metadata": {},
   "source": [
    "### âœ… 11.4.4 ë°ì´í„° ë‚´ë³´ë‚´ê¸°\n",
    "1. **CSV íŒŒì¼ ì €ì¥**\n",
    "2. **JSON íŒŒì¼ ì €ì¥**\n",
    "3. **Excel íŒŒì¼ ì €ì¥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExporter:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "    \n",
    "    def to_csv(self, filename='output.csv'):\n",
    "        \"\"\"CSV íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        self.df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    def to_json(self, filename='output.json'):\n",
    "        \"\"\"JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        self.df.to_json(filename, orient='records', force_ascii=False, indent=4)\n",
    "    \n",
    "    def to_excel(self, filename='output.xlsx'):\n",
    "        \"\"\"Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        self.df.to_excel(filename, index=False)\n",
    "        \n",
    "    def to_html(self, filename='output.html'):\n",
    "        \"\"\"HTML íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        self.df.to_html(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ìŠµ í”„ë¡œì íŠ¸\n",
    "\n",
    "### [ì‹¤ìŠµ 1] ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ê¸°\n",
    "ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ í—¤ë“œë¼ì¸ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "- ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í¬ë¡¤ë§\n",
    "- í‚¤ì›Œë“œ ë¶„ì„\n",
    "- ë°ì´í„° ì‹œê°í™”\n",
    "- ìë™ ë¦¬í¬íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì‹¤ìŠµ í”„ë¡œì íŠ¸ í•´ë‹µ\n",
    "\n",
    "### [ì‹¤ìŠµ 1] ë‰´ìŠ¤ í¬ë¡¤ë§ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news():\n",
    "    url = \"https://news.example.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    news_items = []\n",
    "    articles = soup.select('article.news-item')\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.select_one('h2.title').text.strip()\n",
    "        link = article.select_one('a')['href']\n",
    "        news_items.append({\n",
    "            'title': title,\n",
    "            'link': link\n",
    "        })\n",
    "    \n",
    "    return news_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
