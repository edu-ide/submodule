{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 2권 1장: 웹 스크래핑 (크롤링)\n",
    "\n",
    "## 📌 목차\n",
    "11.1 웹 스크래핑 개념 및 활용  \n",
    "11.2 requests와 BeautifulSoup 사용법  \n",
    "11.3 Selenium을 활용한 동적 웹사이트 크롤링  \n",
    "11.4 크롤링한 데이터 저장 및 분석  \n",
    "\n",
    "## 11.1 웹 스크래핑 개념 및 활용\n",
    "\n",
    "### ✅ 11.1.1 웹 스크래핑의 주요 활용 분야\n",
    "1. **데이터 수집**\n",
    "   - 시장 조사\n",
    "   - 가격 비교\n",
    "   - 트렌드 분석\n",
    "\n",
    "2. **자동화**\n",
    "   - 뉴스 모니터링\n",
    "   - 재고 관리\n",
    "   - 경쟁사 분석\n",
    "\n",
    "3. **연구 및 분석**\n",
    "   - 소셜 미디어 분석\n",
    "   - 여론 조사\n",
    "   - 학술 연구\n",
    "\n",
    "### ✅ 11.1.2 웹 스크래핑 시 주의사항\n",
    "1. **법적 고려사항**\n",
    "   - robots.txt 확인\n",
    "   - 이용약관 준수\n",
    "   - 개인정보 보호\n",
    "\n",
    "2. **기술적 고려사항**\n",
    "   - 요청 간격 조절\n",
    "   - 서버 부하 고려\n",
    "   - 에러 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4870ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"웹 스크래핑을 위한 기본 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def get_page(self, url):\n",
    "        \"\"\"웹 페이지 요청 및 에러 처리\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"에러 발생: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_html(self, html):\n",
    "        \"\"\"HTML 파싱 및 데이터 추출\"\"\"\n",
    "        if html:\n",
    "            return BeautifulSoup(html, 'html.parser')\n",
    "        return None\n",
    "    \n",
    "    def delay_request(self):\n",
    "        \"\"\"요청 간 딜레이 추가\"\"\"\n",
    "        time.sleep(random.uniform(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05002b1b",
   "metadata": {},
   "source": [
    "## 11.2 웹 스크래핑 고급 기법\n",
    "\n",
    "### ✅ 11.2.1 고급 BeautifulSoup 사용법\n",
    "1. **CSS 선택자 활용**\n",
    "2. **정규표현식 결합**\n",
    "3. **계층 구조 탐색**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class AdvancedScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def get_elements_by_css(self, html, selector):\n",
    "        \"\"\"CSS 선택자로 요소 찾기\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup.select(selector)\n",
    "    \n",
    "    def find_by_regex(self, html, pattern):\n",
    "        \"\"\"정규표현식으로 내용 찾기\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup.find(text=re.compile(pattern))\n",
    "    \n",
    "    def navigate_structure(self, element):\n",
    "        \"\"\"요소의 계층 구조 탐색\"\"\"\n",
    "        parent = element.parent\n",
    "        siblings = element.find_next_siblings()\n",
    "        children = element.find_all(recursive=False)\n",
    "        return parent, siblings, children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dd3b0",
   "metadata": {},
   "source": [
    "### ✅ 11.2.2 동적 페이지 처리\n",
    "1. **JavaScript 렌더링 대응**\n",
    "2. **AJAX 요청 처리**\n",
    "3. **무한 스크롤 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class DynamicScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "    \n",
    "    def scroll_to_bottom(self):\n",
    "        \"\"\"무한 스크롤 처리\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "    \n",
    "    def wait_for_element(self, selector):\n",
    "        \"\"\"요소가 로드될 때까지 대기\"\"\"\n",
    "        return self.wait.until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "    \n",
    "    def handle_ajax(self, url):\n",
    "        \"\"\"AJAX 요청 처리\"\"\"\n",
    "        self.driver.get(url)\n",
    "        self.wait_for_element('#content')\n",
    "        return self.driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db5f1f",
   "metadata": {},
   "source": [
    "### ✅ 11.2.3 BeautifulSoup 고급 사용법\n",
    "1. **CSS 선택자 활용**\n",
    "2. **속성 기반 검색**\n",
    "3. **정규표현식 활용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ca048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup 고급 선택자 예제\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def advanced_soup_usage(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # CSS 선택자 사용\n",
    "    articles = soup.select('div.article-content p')\n",
    "    main_content = soup.select('#main-content')\n",
    "    \n",
    "    # 속성으로 찾기\n",
    "    external_links = soup.find_all('a', attrs={'target': '_blank'})\n",
    "    \n",
    "    # 정규표현식으로 찾기\n",
    "    post_divs = soup.find_all('div', class_=re.compile('^post-'))\n",
    "    \n",
    "    return articles, main_content, external_links, post_divs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b2a11",
   "metadata": {},
   "source": [
    "### ✅ 11.2.4 에러 처리와 예외 상황\n",
    "1. **네트워크 오류 처리**\n",
    "2. **타임아웃 설정**\n",
    "3. **재시도 로직**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f01691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import RequestException\n",
    "\n",
    "def safe_get_page(url, retries=3):\n",
    "    \"\"\"안전한 웹 페이지 요청\"\"\"\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except RequestException as e:\n",
    "            print(f\"시도 {i+1}/{retries} 실패: {e}\")\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep(2 ** i)  # 지수 백오프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0090c6b",
   "metadata": {},
   "source": [
    "## 11.3 데이터 저장 및 분석\n",
    "\n",
    "### ✅ 11.3.1 데이터베이스 저장\n",
    "1. **SQLite 활용**\n",
    "2. **MongoDB 활용**\n",
    "3. **데이터 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f53250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class DataStorage:\n",
    "    def __init__(self):\n",
    "        self.sqlite_conn = sqlite3.connect('scraping.db')\n",
    "        self.mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "    \n",
    "    def save_to_sqlite(self, data):\n",
    "        \"\"\"SQLite에 데이터 저장\"\"\"\n",
    "        cursor = self.sqlite_conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles\n",
    "            (title TEXT, url TEXT, content TEXT)\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO articles VALUES (?, ?, ?)\",\n",
    "            [(d['title'], d['url'], d['content']) for d in data]\n",
    "        )\n",
    "        self.sqlite_conn.commit()\n",
    "    \n",
    "    def save_to_mongodb(self, data):\n",
    "        \"\"\"MongoDB에 데이터 저장\"\"\"\n",
    "        db = self.mongo_client.scraping_db\n",
    "        db.articles.insert_many(data)\n",
    "    \n",
    "    def close(self):\n",
    "        self.sqlite_conn.close()\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc76f5",
   "metadata": {},
   "source": [
    "### ✅ 11.3.2 데이터 분석 및 시각화\n",
    "1. **pandas 활용**\n",
    "2. **matplotlib 시각화**\n",
    "3. **워드클라우드 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d6f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "    \n",
    "    def basic_analysis(self):\n",
    "        \"\"\"기본 통계 분석\"\"\"\n",
    "        return {\n",
    "            'total_articles': len(self.df),\n",
    "            'unique_sources': self.df['source'].nunique(),\n",
    "            'date_range': (self.df['date'].min(), self.df['date'].max())\n",
    "        }\n",
    "    \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"데이터 시각화\"\"\"\n",
    "        # 시간별 게시물 수 그래프\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.df['date'].value_counts().sort_index().plot()\n",
    "        plt.title('Posts Over Time')\n",
    "        plt.savefig('posts_timeline.png')\n",
    "        \n",
    "        # 워드클라우드 생성\n",
    "        text = ' '.join(self.df['content'])\n",
    "        wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.savefig('wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520ead3",
   "metadata": {},
   "source": [
    "## 11.4 데이터 저장 및 분석\n",
    "\n",
    "### ✅ 11.4.1 데이터베이스 저장\n",
    "1. **SQLite 활용**\n",
    "2. **MongoDB 활용**\n",
    "3. **데이터 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class DataStorage:\n",
    "    def __init__(self):\n",
    "        self.sqlite_conn = sqlite3.connect('scraping.db')\n",
    "        self.mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "    \n",
    "    def save_to_sqlite(self, data):\n",
    "        \"\"\"SQLite에 데이터 저장\"\"\"\n",
    "        cursor = self.sqlite_conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles\n",
    "            (title TEXT, url TEXT, content TEXT)\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.executemany(\n",
    "            \"INSERT INTO articles VALUES (?, ?, ?)\",\n",
    "            [(d['title'], d['url'], d['content']) for d in data]\n",
    "        )\n",
    "        self.sqlite_conn.commit()\n",
    "    \n",
    "    def save_to_mongodb(self, data):\n",
    "        \"\"\"MongoDB에 데이터 저장\"\"\"\n",
    "        db = self.mongo_client.scraping_db\n",
    "        db.articles.insert_many(data)\n",
    "    \n",
    "    def close(self):\n",
    "        self.sqlite_conn.close()\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6e60c",
   "metadata": {},
   "source": [
    "### ✅ 11.4.2 데이터 분석 및 시각화\n",
    "1. **pandas 활용**\n",
    "2. **matplotlib 시각화**\n",
    "3. **워드클라우드 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "    \n",
    "    def basic_analysis(self):\n",
    "        \"\"\"기본 통계 분석\"\"\"\n",
    "        return {\n",
    "            'total_articles': len(self.df),\n",
    "            'unique_sources': self.df['source'].nunique(),\n",
    "            'date_range': (self.df['date'].min(), self.df['date'].max())\n",
    "        }\n",
    "    \n",
    "    def create_visualizations(self):\n",
    "        \"\"\"데이터 시각화\"\"\"\n",
    "        # 시간별 게시물 수 그래프\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.df['date'].value_counts().sort_index().plot()\n",
    "        plt.title('Posts Over Time')\n",
    "        plt.savefig('posts_timeline.png')\n",
    "        \n",
    "        # 워드클라우드 생성\n",
    "        text = ' '.join(self.df['content'])\n",
    "        wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.savefig('wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832978a",
   "metadata": {},
   "source": [
    "### ✅ 11.4.3 텍스트 데이터 전처리\n",
    "1. **텍스트 정제**\n",
    "2. **불용어 제거**\n",
    "3. **형태소 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.okt = Okt()\n",
    "        self.stopwords = ['은', '는', '이', '가', '을', '를', '의', '에', '에서']\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"텍스트 정제\"\"\"\n",
    "        # HTML 태그 제거\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # 불필요한 공백 제거\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"형태소 분석\"\"\"\n",
    "        # 텍스트 정제\n",
    "        text = self.clean_text(text)\n",
    "        # 형태소 분석\n",
    "        tokens = self.okt.morphs(text)\n",
    "        # 불용어 제거\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c6e26",
   "metadata": {},
   "source": [
    "### ✅ 11.4.4 데이터 내보내기\n",
    "1. **CSV 파일 저장**\n",
    "2. **JSON 파일 저장**\n",
    "3. **Excel 파일 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExporter:\n",
    "    def __init__(self, data):\n",
    "        self.df = pd.DataFrame(data)\n",
    "    \n",
    "    def to_csv(self, filename='output.csv'):\n",
    "        \"\"\"CSV 파일로 저장\"\"\"\n",
    "        self.df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    def to_json(self, filename='output.json'):\n",
    "        \"\"\"JSON 파일로 저장\"\"\"\n",
    "        self.df.to_json(filename, orient='records', force_ascii=False, indent=4)\n",
    "    \n",
    "    def to_excel(self, filename='output.xlsx'):\n",
    "        \"\"\"Excel 파일로 저장\"\"\"\n",
    "        self.df.to_excel(filename, index=False)\n",
    "        \n",
    "    def to_html(self, filename='output.html'):\n",
    "        \"\"\"HTML 파일로 저장\"\"\"\n",
    "        self.df.to_html(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 실습 프로젝트\n",
    "\n",
    "### [실습 1] 뉴스 헤드라인 수집기\n",
    "주요 뉴스 사이트의 헤드라인을 자동으로 수집하고 분석하는 프로그램을 작성하세요.\n",
    "- 여러 뉴스 사이트 크롤링\n",
    "- 키워드 분석\n",
    "- 데이터 시각화\n",
    "- 자동 리포트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 실습 프로젝트 해답\n",
    "\n",
    "### [실습 1] 뉴스 크롤링 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_news():\n",
    "    url = \"https://news.example.com\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    news_items = []\n",
    "    articles = soup.select('article.news-item')\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.select_one('h2.title').text.strip()\n",
    "        link = article.select_one('a')['href']\n",
    "        news_items.append({\n",
    "            'title': title,\n",
    "            'link': link\n",
    "        })\n",
    "    \n",
    "    return news_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
