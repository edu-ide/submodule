# 5.5 실습 프로젝트

## 🎯 대용량 데이터 병렬 처리

```python
def process_large_dataset():
    """대용량 데이터 병렬 처리 예제"""
    # 데이터 생성
    data = [(i, i * 2) for i in range(10**6)]
    
    # 병렬 처리기 초기화
    processor = ParallelProcessor()
    
    # 데이터 처리 함수
    def process_item(item):
        x, y = item
        return x * y + sum(i * i for i in range(1000))
    
    # 멀티프로세싱으로 처리
    results, time_taken = processor.benchmark(
        processor.process_pool_execution,
        process_item,
        data
    )
    
    print(f"처리 완료: {len(results):,}개 항목")
    print(f"소요 시간: {time_taken:.2f}초")
    
    return results
```

## 🎯 이미지 처리 성능 최적화

```python
def image_processing_optimization():
    """이미지 처리 성능 최적화 프로젝트"""
    try:
        import numpy as np
        from PIL import Image
        import time
        
        # 이미지 처리 함수들
        def apply_blur_python(image_data, kernel_size=3):
            """순수 파이썬으로 블러 필터 적용 (느림)"""
            height, width = len(image_data), len(image_data[0])
            result = [[0 for _ in range(width)] for _ in range(height)]
            
            pad = kernel_size // 2
            for i in range(pad, height - pad):
                for j in range(pad, width - pad):
                    # 커널 영역의 평균 계산
                    sum_val = 0
                    count = 0
                    for ki in range(-pad, pad + 1):
                        for kj in range(-pad, pad + 1):
                            sum_val += image_data[i + ki][j + kj]
                            count += 1
                    result[i][j] = sum_val // count
            
            return result
        
        def apply_blur_numpy(image_array, kernel_size=3):
            """NumPy로 블러 필터 적용 (빠름)"""
            from scipy.ndimage import uniform_filter
            return uniform_filter(image_array, size=kernel_size)
        
        # 임의 이미지 데이터 생성 (실제 애플리케이션에서는 이미지 파일 로드)
        img_size = 500
        image_data = [[np.random.randint(0, 256) for _ in range(img_size)] for _ in range(img_size)]
        image_array = np.array(image_data, dtype=np.uint8)
        
        # 성능 측정 - 파이썬 버전
        start = time.time()
        py_result = apply_blur_python(image_data, 3)
        py_time = time.time() - start
        
        # 성능 측정 - NumPy 버전
        start = time.time()
        np_result = apply_blur_numpy(image_array, 3)
        np_time = time.time() - start
        
        print(f"파이썬 처리 시간: {py_time:.4f}초")
        print(f"NumPy 처리 시간: {np_time:.4f}초")
        print(f"속도 향상: {py_time / np_time:.1f}배")
        
    except ImportError:
        print("필요한 패키지가 설치되어 있지 않습니다. 다음을 설치하세요:")
        print("pip install numpy pillow scipy")
```

## 🎯 웹 스크래핑 비동기 처리

```python
def async_web_scraping():
    """웹 스크래핑 비동기 처리 프로젝트"""
    import asyncio
    import aiohttp
    import time
    from bs4 import BeautifulSoup
    import re
    
    # 동기 방식 스크래핑
    def sync_scrape(urls):
        """동기적 웹 스크래핑"""
        import requests
        
        results = []
        start = time.time()
        
        for url in urls:
            try:
                response = requests.get(url, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.title.string if soup.title else "제목 없음"
                results.append({"url": url, "title": title})
            except Exception as e:
                results.append({"url": url, "error": str(e)})
        
        elapsed = time.time() - start
        return results, elapsed
    
    # 비동기 방식 스크래핑
    async def async_scrape(urls):
        """비동기적 웹 스크래핑"""
        results = []
        start = time.time()
        
        async def fetch(session, url):
            try:
                async with session.get(url, timeout=10) as response:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    title = soup.title.string if soup.title else "제목 없음"
                    return {"url": url, "title": title}
            except Exception as e:
                return {"url": url, "error": str(e)}
        
        async with aiohttp.ClientSession() as session:
            tasks = [fetch(session, url) for url in urls]
            results = await asyncio.gather(*tasks)
        
        elapsed = time.time() - start
        return results, elapsed
    
    # 테스트 URL 리스트 (공개 웹사이트)
    test_urls = [
        "https://www.python.org",
        "https://docs.python.org",
        "https://pypi.org",
        "https://www.wikipedia.org",
        "https://www.github.com"
    ]
    
    # 동기 방식 테스트
    sync_results, sync_time = sync_scrape(test_urls)
    
    # 비동기 방식 테스트
    async_results, async_time = asyncio.run(async_scrape(test_urls))
    
    # 결과 출력
    print("[ 동기 방식 결과 ]")
    for result in sync_results:
        if "error" in result:
            print(f"{result['url']}: 오류 - {result['error']}")
        else:
            print(f"{result['url']}: {result['title']}")
    print(f"소요 시간: {sync_time:.2f}초")
    
    print("\n[ 비동기 방식 결과 ]")
    for result in async_results:
        if "error" in result:
            print(f"{result['url']}: 오류 - {result['error']}")
        else:
            print(f"{result['url']}: {result['title']}")
    print(f"소요 시간: {async_time:.2f}초")
    print(f"속도 향상: {sync_time / async_time:.1f}배")
```

## 🎯 성능 최적화 종합 프로젝트

```python
def comprehensive_optimization_project():
    """성능 최적화 종합 프로젝트"""
    
    class DataProcessor:
        """데이터 처리 및 최적화 클래스"""
        
        def __init__(self, data_size=1000000):
            self.data_size = data_size
            self.generate_data()
        
        def generate_data(self):
            """대용량 데이터 생성"""
            import random
            self.data = [random.randint(1, 1000) for _ in range(self.data_size)]
        
        def process_sequential(self):
            """순차적 처리"""
            result = []
            for item in self.data:
                if item % 2 == 0:  # 짝수만 처리
                    result.append(self._transform_item(item))
            return result
        
        def process_functional(self):
            """함수형 처리"""
            return list(map(self._transform_item, filter(lambda x: x % 2 == 0, self.data)))
        
        def process_comprehension(self):
            """컴프리헨션 처리"""
            return [self._transform_item(item) for item in self.data if item % 2 == 0]
        
        def process_parallel(self):
            """병렬 처리"""
            from concurrent.futures import ProcessPoolExecutor
            
            # 짝수만 필터링
            even_data = [item for item in self.data if item % 2 == 0]
            
            # 병렬 처리
            with ProcessPoolExecutor() as executor:
                return list(executor.map(self._transform_item, even_data))
        
        def _transform_item(self, item):
            """아이템 변환 (계산 집약적 작업)"""
            return sum(i * i for i in range(item))
    
    # 실행 및 비교
    import time
    
    processor = DataProcessor(data_size=100000)
    
    # 순차적 처리
    start = time.time()
    seq_result = processor.process_sequential()
    seq_time = time.time() - start
    
    # 함수형 처리
    start = time.time()
    func_result = processor.process_functional()
    func_time = time.time() - start
    
    # 컴프리헨션 처리
    start = time.time()
    comp_result = processor.process_comprehension()
    comp_time = time.time() - start
    
    # 병렬 처리
    start = time.time()
    par_result = processor.process_parallel()
    par_time = time.time() - start
    
    # 결과 출력
    print(f"처리된 항목 수: {len(seq_result):,}")
    print(f"순차적 처리 시간: {seq_time:.4f}초")
    print(f"함수형 처리 시간: {func_time:.4f}초 (속도 향상: {seq_time/func_time:.1f}배)")
    print(f"컴프리헨션 처리 시간: {comp_time:.4f}초 (속도 향상: {seq_time/comp_time:.1f}배)")
    print(f"병렬 처리 시간: {par_time:.4f}초 (속도 향상: {seq_time/par_time:.1f}배)")
    
    # 검증 (모든 결과가 동일한지 확인)
    results_match = (
        len(seq_result) == len(func_result) == len(comp_result) == len(par_result) and
        all(seq_result[i] == func_result[i] == comp_result[i] == par_result[i]
            for i in range(min(10, len(seq_result))))
    )
    
    print(f"결과 일치 여부: {'예' if results_match else '아니오'}")
```

## 🎯 추가 도전 과제

1. **분산 처리 시스템 구현**: Dask 또는 Ray를 사용하여 여러 머신에 걸친 분산 처리 시스템 구현해보기

2. **GPU 가속화**: CUDA와 함께 Numba를 사용하여 GPU로 가속화된 계산 수행하기

3. **실시간 데이터 파이프라인**: 비동기 처리와 멀티프로세싱을 결합하여 실시간 데이터 처리 파이프라인 구축하기

4. **데이터베이스 쿼리 최적화**: ORM과 원시 SQL을 비교하고, 데이터베이스 쿼리 성능을 최적화하는 방법 연구하기

5. **메모리 사용량 최적화**: 대규모 데이터셋을 처리할 때 메모리 사용량을 최소화하는 전략 개발하기 