---

# ğŸ“˜ ì›¹ ìŠ¤í¬ë˜í•‘ (í¬ë¡¤ë§) - ì‹¤ìŠµ í”„ë¡œì íŠ¸

## ğŸ¯ ì‹¤ìŠµ í”„ë¡œì íŠ¸

### [ì‹¤ìŠµ 1] ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ê¸°
ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ í—¤ë“œë¼ì¸ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
- í‚¤ì›Œë“œ ë¶„ì„
- ë°ì´í„° ì‹œê°í™”
- ìë™ ë¦¬í¬íŠ¸ ìƒì„±

### [ì‹¤ìŠµ 1] ë‰´ìŠ¤ í¬ë¡¤ë§ êµ¬í˜„

```python
def crawl_news():
    url = "https://news.example.com"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    
    news_items = []
    articles = soup.select('article.news-item')
    
    for article in articles:
        title = article.select_one('h2.title').text.strip()
        link = article.select_one('a')['href']
        news_items.append({
            'title': title,
            'link': link
        })
    
    return news_items
``` 