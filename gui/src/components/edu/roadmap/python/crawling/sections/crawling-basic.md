---

# ğŸ“˜ ì›¹ ìŠ¤í¬ë˜í•‘ (í¬ë¡¤ë§) - ê¸°ë³¸ ë„êµ¬ í™œìš©

## 11.2 ì›¹ ìŠ¤í¬ë˜í•‘ ê³ ê¸‰ ê¸°ë²•

### âœ… 11.2.1 ê³ ê¸‰ BeautifulSoup ì‚¬ìš©ë²•
1. **CSS ì„ íƒì í™œìš©**
2. **ì •ê·œí‘œí˜„ì‹ ê²°í•©**
3. **ê³„ì¸µ êµ¬ì¡° íƒìƒ‰**

```python
import requests
from bs4 import BeautifulSoup
import re

class AdvancedScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def get_elements_by_css(self, html, selector):
        """CSS ì„ íƒìë¡œ ìš”ì†Œ ì°¾ê¸°"""
        soup = BeautifulSoup(html, 'html.parser')
        return soup.select(selector)
    
    def find_by_regex(self, html, pattern):
        """ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë‚´ìš© ì°¾ê¸°"""
        soup = BeautifulSoup(html, 'html.parser')
        return soup.find(text=re.compile(pattern))
    
    def navigate_structure(self, element):
        """ìš”ì†Œì˜ ê³„ì¸µ êµ¬ì¡° íƒìƒ‰"""
        parent = element.parent
        siblings = element.find_next_siblings()
        children = element.find_all(recursive=False)
        return parent, siblings, children
```

### âœ… 11.2.3 BeautifulSoup ê³ ê¸‰ ì‚¬ìš©ë²•
1. **CSS ì„ íƒì í™œìš©**
2. **ì†ì„± ê¸°ë°˜ ê²€ìƒ‰**
3. **ì •ê·œí‘œí˜„ì‹ í™œìš©**

```python
# BeautifulSoup ê³ ê¸‰ ì„ íƒì ì˜ˆì œ
import re
from bs4 import BeautifulSoup

def advanced_soup_usage(html):
    soup = BeautifulSoup(html, 'html.parser')
    
    # CSS ì„ íƒì ì‚¬ìš©
    articles = soup.select('div.article-content p')
    main_content = soup.select('#main-content')
    
    # ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
    external_links = soup.find_all('a', attrs={'target': '_blank'})
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ê¸°
    post_divs = soup.find_all('div', class_=re.compile('^post-'))
    
    return articles, main_content, external_links, post_divs
```

### âœ… 11.2.4 ì—ëŸ¬ ì²˜ë¦¬ì™€ ì˜ˆì™¸ ìƒí™©
1. **ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬**
2. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**
3. **ì¬ì‹œë„ ë¡œì§**

```python
from requests.exceptions import RequestException

def safe_get_page(url, retries=3):
    """ì•ˆì „í•œ ì›¹ í˜ì´ì§€ ìš”ì²­"""
    for i in range(retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except RequestException as e:
            print(f"ì‹œë„ {i+1}/{retries} ì‹¤íŒ¨: {e}")
            if i == retries - 1:
                raise
            time.sleep(2 ** i)  # ì§€ìˆ˜ ë°±ì˜¤í”„
``` 