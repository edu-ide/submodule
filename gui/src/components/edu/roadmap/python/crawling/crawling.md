---

# ğŸ“˜ 2ê¶Œ 1ì¥: ì›¹ ìŠ¤í¬ë˜í•‘ (í¬ë¡¤ë§)

## ğŸ“Œ ëª©ì°¨
11.1 ì›¹ ìŠ¤í¬ë˜í•‘ ê°œë… ë° í™œìš©  
11.2 requestsì™€ BeautifulSoup ì‚¬ìš©ë²•  
11.3 Seleniumì„ í™œìš©í•œ ë™ì  ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§  
11.4 í¬ë¡¤ë§í•œ ë°ì´í„° ì €ì¥ ë° ë¶„ì„  

## 11.1 ì›¹ ìŠ¤í¬ë˜í•‘ ê°œë… ë° í™œìš©

### âœ… 11.1.1 ì›¹ ìŠ¤í¬ë˜í•‘ì˜ ì£¼ìš” í™œìš© ë¶„ì•¼
1. **ë°ì´í„° ìˆ˜ì§‘**
   - ì‹œì¥ ì¡°ì‚¬
   - ê°€ê²© ë¹„êµ
   - íŠ¸ë Œë“œ ë¶„ì„
2. **ìë™í™”**
   - ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§
   - ì¬ê³  ê´€ë¦¬
   - ê²½ìŸì‚¬ ë¶„ì„
3. **ì—°êµ¬ ë° ë¶„ì„**
   - ì†Œì…œ ë¯¸ë””ì–´ ë¶„ì„
   - ì—¬ë¡  ì¡°ì‚¬
   - í•™ìˆ  ì—°êµ¬

### âœ… 11.1.2 ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ ì£¼ì˜ì‚¬í•­
1. **ë²•ì  ê³ ë ¤ì‚¬í•­**
   - robots.txt í™•ì¸
   - ì´ìš©ì•½ê´€ ì¤€ìˆ˜
   - ê°œì¸ì •ë³´ ë³´í˜¸
2. **ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­**
   - ìš”ì²­ ê°„ê²© ì¡°ì ˆ
   - ì„œë²„ ë¶€í•˜ ê³ ë ¤
   - ì—ëŸ¬ ì²˜ë¦¬

```python
import requests
from bs4 import BeautifulSoup
import time
import random

class WebScraper:
    """ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url):
        self.base_url = base_url
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def get_page(self, url):
        """ì›¹ í˜ì´ì§€ ìš”ì²­ ë° ì—ëŸ¬ ì²˜ë¦¬"""
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"ì—ëŸ¬ ë°œìƒ: {e}")
            return None
    
    def parse_html(self, html):
        """HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ"""
        if html:
            return BeautifulSoup(html, 'html.parser')
        return None
    
    def delay_request(self):
        """ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€"""
        time.sleep(random.uniform(1, 3))
```

## 11.2 ì›¹ ìŠ¤í¬ë˜í•‘ ê³ ê¸‰ ê¸°ë²•

### âœ… 11.2.1 ê³ ê¸‰ BeautifulSoup ì‚¬ìš©ë²•
1. **CSS ì„ íƒì í™œìš©**
2. **ì •ê·œí‘œí˜„ì‹ ê²°í•©**
3. **ê³„ì¸µ êµ¬ì¡° íƒìƒ‰**

```python
import requests
from bs4 import BeautifulSoup
import re

class AdvancedScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def get_elements_by_css(self, html, selector):
        """CSS ì„ íƒìë¡œ ìš”ì†Œ ì°¾ê¸°"""
        soup = BeautifulSoup(html, 'html.parser')
        return soup.select(selector)
    
    def find_by_regex(self, html, pattern):
        """ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë‚´ìš© ì°¾ê¸°"""
        soup = BeautifulSoup(html, 'html.parser')
        return soup.find(text=re.compile(pattern))
    
    def navigate_structure(self, element):
        """ìš”ì†Œì˜ ê³„ì¸µ êµ¬ì¡° íƒìƒ‰"""
        parent = element.parent
        siblings = element.find_next_siblings()
        children = element.find_all(recursive=False)
        return parent, siblings, children
```

### âœ… 11.2.2 ë™ì  í˜ì´ì§€ ì²˜ë¦¬
1. **JavaScript ë Œë”ë§ ëŒ€ì‘**
2. **AJAX ìš”ì²­ ì²˜ë¦¬**
3. **ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class DynamicScraper:
    def __init__(self):
        self.driver = webdriver.Chrome()
        self.wait = WebDriverWait(self.driver, 10)
    
    def scroll_to_bottom(self):
        """ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    
    def wait_for_element(self, selector):
        """ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        return self.wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
        )
    
    def handle_ajax(self, url):
        """AJAX ìš”ì²­ ì²˜ë¦¬"""
        self.driver.get(url)
        self.wait_for_element('#content')
        return self.driver.page_source
```

### âœ… 11.2.3 BeautifulSoup ê³ ê¸‰ ì‚¬ìš©ë²•
1. **CSS ì„ íƒì í™œìš©**
2. **ì†ì„± ê¸°ë°˜ ê²€ìƒ‰**
3. **ì •ê·œí‘œí˜„ì‹ í™œìš©**

```python
# BeautifulSoup ê³ ê¸‰ ì„ íƒì ì˜ˆì œ
import re
from bs4 import BeautifulSoup

def advanced_soup_usage(html):
    soup = BeautifulSoup(html, 'html.parser')
    
    # CSS ì„ íƒì ì‚¬ìš©
    articles = soup.select('div.article-content p')
    main_content = soup.select('#main-content')
    
    # ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
    external_links = soup.find_all('a', attrs={'target': '_blank'})
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì°¾ê¸°
    post_divs = soup.find_all('div', class_=re.compile('^post-'))
    
    return articles, main_content, external_links, post_divs
```

### âœ… 11.2.4 ì—ëŸ¬ ì²˜ë¦¬ì™€ ì˜ˆì™¸ ìƒí™©
1. **ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬**
2. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**
3. **ì¬ì‹œë„ ë¡œì§**

```python
from requests.exceptions import RequestException

def safe_get_page(url, retries=3):
    """ì•ˆì „í•œ ì›¹ í˜ì´ì§€ ìš”ì²­"""
    for i in range(retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except RequestException as e:
            print(f"ì‹œë„ {i+1}/{retries} ì‹¤íŒ¨: {e}")
            if i == retries - 1:
                raise
            time.sleep(2 ** i)  # ì§€ìˆ˜ ë°±ì˜¤í”„
```

## 11.3 ë°ì´í„° ì €ì¥ ë° ë¶„ì„

### âœ… 11.3.1 ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
1. **SQLite í™œìš©**
2. **MongoDB í™œìš©**
3. **ë°ì´í„° ì •ê·œí™”**

```python
import sqlite3
from pymongo import MongoClient

class DataStorage:
    def __init__(self):
        self.sqlite_conn = sqlite3.connect('scraping.db')
        self.mongo_client = MongoClient('mongodb://localhost:27017/')
    
    def save_to_sqlite(self, data):
        """SQLiteì— ë°ì´í„° ì €ì¥"""
        cursor = self.sqlite_conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS articles
            (title TEXT, url TEXT, content TEXT)
        """)
        
        cursor.executemany(
            "INSERT INTO articles VALUES (?, ?, ?)",
            [(d['title'], d['url'], d['content']) for d in data]
        )
        self.sqlite_conn.commit()
    
    def save_to_mongodb(self, data):
        """MongoDBì— ë°ì´í„° ì €ì¥"""
        db = self.mongo_client.scraping_db
        db.articles.insert_many(data)
    
    def close(self):
        self.sqlite_conn.close()
        self.mongo_client.close()
```

### âœ… 11.3.2 ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”
1. **pandas í™œìš©**
2. **matplotlib ì‹œê°í™”**
3. **ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±**

```python
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud

class DataAnalyzer:
    def __init__(self, data):
        self.df = pd.DataFrame(data)
    
    def basic_analysis(self):
        """ê¸°ë³¸ í†µê³„ ë¶„ì„"""
        return {
            'total_articles': len(self.df),
            'unique_sources': self.df['source'].nunique(),
            'date_range': (self.df['date'].min(), self.df['date'].max())
        }
    
    def create_visualizations(self):
        """ë°ì´í„° ì‹œê°í™”"""
        # ì‹œê°„ë³„ ê²Œì‹œë¬¼ ìˆ˜ ê·¸ë˜í”„
        plt.figure(figsize=(12, 6))
        self.df['date'].value_counts().sort_index().plot()
        plt.title('Posts Over Time')
        plt.savefig('posts_timeline.png')
        
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        text = ' '.join(self.df['content'])
        wordcloud = WordCloud(width=800, height=400).generate(text)
        
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.savefig('wordcloud.png')
```

### âœ… 11.4.3 í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
1. **í…ìŠ¤íŠ¸ ì •ì œ**
2. **ë¶ˆìš©ì–´ ì œê±°**
3. **í˜•íƒœì†Œ ë¶„ì„**

```python
import re
from konlpy.tag import Okt

class TextPreprocessor:
    def __init__(self):
        self.okt = Okt()
        self.stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ']
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s]', '', text)
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = ' '.join(text.split())
        return text
    
    def tokenize(self, text):
        """í˜•íƒœì†Œ ë¶„ì„"""
        # í…ìŠ¤íŠ¸ ì •ì œ
        text = self.clean_text(text)
        # í˜•íƒœì†Œ ë¶„ì„
        tokens = self.okt.morphs(text)
        # ë¶ˆìš©ì–´ ì œê±°
        tokens = [token for token in tokens if token not in self.stopwords]
        return tokens
```

### âœ… 11.4.4 ë°ì´í„° ë‚´ë³´ë‚´ê¸°
1. **CSV íŒŒì¼ ì €ì¥**
2. **JSON íŒŒì¼ ì €ì¥**
3. **Excel íŒŒì¼ ì €ì¥**

```python
class DataExporter:
    def __init__(self, data):
        self.df = pd.DataFrame(data)
    
    def to_csv(self, filename='output.csv'):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        self.df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    def to_json(self, filename='output.json'):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        self.df.to_json(filename, orient='records', force_ascii=False, indent=4)
    
    def to_excel(self, filename='output.xlsx'):
        """Excel íŒŒì¼ë¡œ ì €ì¥"""
        self.df.to_excel(filename, index=False)
        
    def to_html(self, filename='output.html'):
        """HTML íŒŒì¼ë¡œ ì €ì¥"""
        self.df.to_html(filename)
```

## ğŸ¯ ì‹¤ìŠµ í”„ë¡œì íŠ¸

### [ì‹¤ìŠµ 1] ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ê¸°
ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì˜ í—¤ë“œë¼ì¸ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
- í‚¤ì›Œë“œ ë¶„ì„
- ë°ì´í„° ì‹œê°í™”
- ìë™ ë¦¬í¬íŠ¸ ìƒì„±

### [ì‹¤ìŠµ 1] ë‰´ìŠ¤ í¬ë¡¤ë§ êµ¬í˜„

```python
def crawl_news():
    url = "https://news.example.com"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    
    news_items = []
    articles = soup.select('article.news-item')
    
    for article in articles:
        title = article.select_one('h2.title').text.strip()
        link = article.select_one('a')['href']
        news_items.append({
            'title': title,
            'link': link
        })
    
    return news_items
```

---