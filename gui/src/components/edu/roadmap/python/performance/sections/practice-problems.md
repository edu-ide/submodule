---

# ğŸ“˜ 10ì¥ ì‹¤ìŠµ ë¬¸ì œ

ì´ ì¥ì—ì„œ ë°°ìš´ ì„±ëŠ¥ ìµœì í™” ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒ ë¬¸ì œë“¤ì„ í•´ê²°í•´ ë³´ì„¸ìš”.

## âœ… ë¬¸ì œ 1: ì½”ë“œ í”„ë¡œíŒŒì¼ë§

ë‹¤ìŒ ì½”ë“œì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ê°œì„ í•˜ì„¸ìš”.

```python
# ìµœì í™”ê°€ í•„ìš”í•œ ì½”ë“œ
def find_primes(n):
    """nê¹Œì§€ì˜ ì†Œìˆ˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
    primes = []
    for num in range(2, n + 1):
        is_prime = True
        for i in range(2, num):
            if num % i == 0:
                is_prime = False
                break
        if is_prime:
            primes.append(num)
    return primes

# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    import time
    
    start = time.time()
    primes = find_primes(10000)
    end = time.time()
    
    print(f"ì‹¤í–‰ ì‹œê°„: {end - start:.6f}ì´ˆ")
    print(f"ì°¾ì€ ì†Œìˆ˜ ê°œìˆ˜: {len(primes)}")
```

**ìš”êµ¬ì‚¬í•­:**
1. cProfileì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ í”„ë¡œíŒŒì¼ë§í•˜ì„¸ìš”.
2. ì—ë¼í† ìŠ¤í…Œë„¤ìŠ¤ì˜ ì²´ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ëœ ë²„ì „ì„ ì‘ì„±í•˜ì„¸ìš”.
3. ìµœì í™” ì „í›„ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## âœ… ë¬¸ì œ 2: ë©”ëª¨ë¦¬ ìµœì í™”

ë‹¤ìŒ ì½”ë“œëŠ” ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•˜ì„¸ìš”.

```python
def process_large_file(filename):
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ê° ì¤„ì— ëŒ€í•´ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    # íŒŒì¼ì˜ ëª¨ë“  ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    with open(filename, 'r') as file:
        lines = file.readlines()
    
    # ê° ì¤„ ì²˜ë¦¬
    processed_lines = []
    for line in lines:
        # ê³µë°± ì œê±° ë° ëŒ€ë¬¸ìë¡œ ë³€í™˜
        processed_line = line.strip().upper()
        processed_lines.append(processed_line)
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    with open('processed_' + filename, 'w') as output_file:
        output_file.write('\n'.join(processed_lines))
```

**ìš”êµ¬ì‚¬í•­:**
1. memory_profilerë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í”„ë¡œíŒŒì¼ë§í•˜ì„¸ìš”.
2. íŒŒì¼ì„ í•œ ë²ˆì— ëª¨ë‘ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•Šê³  ì¤„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
3. ì œë„ˆë ˆì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë” íš¨ìœ¨ì ì¸ ë°©ë²•ìœ¼ë¡œ ë¦¬íŒ©í† ë§í•˜ì„¸ìš”.
4. ìµœì í™” ì „í›„ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¹„êµí•˜ì„¸ìš”.

## âœ… ë¬¸ì œ 3: ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”

ë‹¤ìŒ ì½”ë“œëŠ” URL ëª©ë¡ì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì—¬ ìµœì í™”í•˜ì„¸ìš”.

```python
import requests
import time

def download_and_process(urls):
    """URL ëª©ë¡ì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    results = []
    
    for url in urls:
        # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        response = requests.get(url)
        data = response.text
        
        # ë°ì´í„° ì²˜ë¦¬ (ê°„ë‹¨í•œ ì˜ˆ)
        word_count = len(data.split())
        results.append((url, word_count))
    
    return results

# í…ŒìŠ¤íŠ¸ URL ëª©ë¡
test_urls = [
    'http://example.com',
    'http://example.org',
    'http://example.net',
    'https://python.org',
    'https://pypi.org'
]

# ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •
start = time.time()
results = download_and_process(test_urls)
end = time.time()

print(f"ì´ ì‹¤í–‰ ì‹œê°„: {end - start:.6f}ì´ˆ")
for url, count in results:
    print(f"{url}: {count}ê°œ ë‹¨ì–´")
```

**ìš”êµ¬ì‚¬í•­:**
1. ë©€í‹°ìŠ¤ë ˆë”©ì„ ì‚¬ìš©í•˜ì—¬ URL ë‹¤ìš´ë¡œë“œë¥¼ ë³‘ë ¬í™”í•˜ì„¸ìš”.
2. ë™ì¼í•œ ì½”ë“œë¥¼ asyncioì™€ aiohttpë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ì„¸ìš”.
3. ìˆœì°¨ ì²˜ë¦¬, ë©€í‹°ìŠ¤ë ˆë“œ, ë¹„ë™ê¸° ë°©ì‹ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.

## âœ… ë¬¸ì œ 4: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

íŒŒì´ì¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê°„ë‹¨í•œ ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“œì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. ë‹¤ìŒ ì§€í‘œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§:
   - CPU ì‚¬ìš©ëŸ‰
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - ë””ìŠ¤í¬ I/O
   - ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½
2. ë°ì´í„°ë¥¼ 10ì´ˆ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ CSV íŒŒì¼ì— ì €ì¥
3. matplotlib ë˜ëŠ” ë‹¤ë¥¸ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¡œ í‘œì‹œ
4. (ì„ íƒ) ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì œê³µ (Flask ë˜ëŠ” Dash í™œìš©)

## âœ… ë¬¸ì œ 5: ì½”ë“œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.

```python
# ë¦¬ìŠ¤íŠ¸ ìƒì„± ë°©ë²• ë¹„êµ
def create_list_method1(n):
    """for ë£¨í”„ ì‚¬ìš©"""
    result = []
    for i in range(n):
        result.append(i * i)
    return result

def create_list_method2(n):
    """ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì‚¬ìš©"""
    return [i * i for i in range(n)]

def create_list_method3(n):
    """map í•¨ìˆ˜ ì‚¬ìš©"""
    return list(map(lambda x: x * x, range(n)))

# ë”•ì…”ë„ˆë¦¬ ìƒì„± ë°©ë²• ë¹„êµ
def create_dict_method1(n):
    """for ë£¨í”„ ì‚¬ìš©"""
    result = {}
    for i in range(n):
        result[i] = i * i
    return result

def create_dict_method2(n):
    """ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ ì‚¬ìš©"""
    return {i: i * i for i in range(n)}

def create_dict_method3(n):
    """zip í™œìš©"""
    keys = range(n)
    values = map(lambda x: x * x, range(n))
    return dict(zip(keys, values))
```

**ìš”êµ¬ì‚¬í•­:**
1. timeit ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ê° í•¨ìˆ˜ì˜ ì‹¤í–‰ ì‹œê°„ì„ ì¸¡ì •í•˜ì„¸ìš”.
2. ë‹¤ì–‘í•œ ì…ë ¥ í¬ê¸°(n)ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.
3. ê²°ê³¼ë¥¼ í‘œì™€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì„¸ìš”.
4. ì‹¤í–‰ í™˜ê²½ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•˜ì„¸ìš” (ì˜ˆ: íŒŒì´ì¬ ë²„ì „, OS ë“±).

## âœ… ë¬¸ì œ 6: ì¢…í•© ìµœì í™” í”„ë¡œì íŠ¸

ì›¹ ìŠ¤í¬ë˜í•‘ê³¼ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìµœì í™”í•˜ì„¸ìš”.

```python
# ì›¹ ìŠ¤í¬ë˜í•‘ ë° ë°ì´í„° ì²˜ë¦¬ ì• í”Œë¦¬ì¼€ì´ì…˜
import requests
from bs4 import BeautifulSoup
import csv
import time

def scrape_websites(urls):
    """ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì œëª©ê³¼ ë§í¬ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    all_data = []
    
    for url in urls:
        # ì›¹ì‚¬ì´íŠ¸ ì ‘ì†
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ (ê° ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
        titles = soup.find_all('h2')
        links = soup.find_all('a')
        
        # ë°ì´í„° ì €ì¥
        website_data = {
            'url': url,
            'titles': [title.text.strip() for title in titles],
            'links': [link.get('href') for link in links if link.get('href')]
        }
        all_data.append(website_data)
        
        # ì›¹ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
        time.sleep(1)
    
    return all_data

def process_data(all_data):
    """ìŠ¤í¬ë˜í•‘í•œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    with open('scraped_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Website', 'Title', 'Link'])
        
        for website_data in all_data:
            url = website_data['url']
            titles = website_data['titles']
            links = website_data['links']
            
            # ì œëª©ê³¼ ë§í¬ ë°ì´í„° ì²˜ë¦¬
            for title in titles:
                writer.writerow([url, title, ''])
            
            for link in links:
                writer.writerow([url, '', link])

# í…ŒìŠ¤íŠ¸ URL ëª©ë¡
test_urls = [
    'https://example.com',
    'https://example.org',
    'https://python.org',
    'https://pypi.org',
    'https://docs.python.org'
]

# ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •
start = time.time()
data = scrape_websites(test_urls)
process_data(data)
end = time.time()

print(f"ì´ ì‹¤í–‰ ì‹œê°„: {end - start:.6f}ì´ˆ")
```

**ìš”êµ¬ì‚¬í•­:**
1. ì½”ë“œ í”„ë¡œíŒŒì¼ë§ì„ í†µí•´ ì„±ëŠ¥ ë³‘ëª© ì§€ì ì„ ì°¾ìœ¼ì„¸ìš”.
2. ë‹¤ìŒ ë°©ë²•ë“¤ì„ ì ìš©í•˜ì—¬ ì½”ë“œë¥¼ ìµœì í™”í•˜ì„¸ìš”:
   - ë©€í‹°ìŠ¤ë ˆë”© ë˜ëŠ” ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘ ë³‘ë ¬í™”
   - ë©”ëª¨ë¦¬ ìµœì í™” (ë¶ˆí•„ìš”í•œ ì¤‘ê°„ ë°ì´í„° ì €ì¥ ë°©ì§€)
   - ì œë„ˆë ˆì´í„°ë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
   - CSV ì“°ê¸° ìµœì í™”
3. ëª¨ë‹ˆí„°ë§ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ CPUì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ì„¸ìš”.
4. ìµœì í™” ì „í›„ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

--- 