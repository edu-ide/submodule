# 6.3 딥러닝 기초

## ✅ 6.3.1 인공 신경망의 구조

인공 신경망은 인간 뇌의 뉴런 구조에서 영감을 받은 머신러닝 모델입니다. 기본 구성 요소는 다음과 같습니다:

1. **뉴런(Neuron)**: 입력을 받아 가중 합을 계산하고 활성화 함수를 통해 출력을 생성하는 기본 단위
2. **계층(Layer)**: 여러 뉴런으로 구성된 층
   - 입력층(Input Layer): 데이터를 받는 첫 번째 층
   - 은닉층(Hidden Layer): 입력층과 출력층 사이의 모든 층
   - 출력층(Output Layer): 최종 예측을 생성하는 층
3. **가중치(Weight)**: 입력과 뉴런 간의 연결 강도를 나타내는 파라미터
4. **편향(Bias)**: 뉴런의 활성화 임계값을 조정하는 상수
5. **활성화 함수(Activation Function)**: 비선형성을 추가하여 복잡한 패턴 학습 가능

```python
import numpy as np

def simple_neuron():
    """단일 뉴런 구현 예제"""
    # 입력 값
    inputs = np.array([1.0, 2.0, 3.0])
    
    # 가중치와 편향
    weights = np.array([0.2, 0.3, 0.5])
    bias = 0.1
    
    # 가중합 계산
    weighted_sum = np.dot(inputs, weights) + bias
    
    # 활성화 함수 (시그모이드)
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
    
    # 출력
    output = sigmoid(weighted_sum)
    print(f"뉴런 출력: {output:.4f}")
    
    return output
```

## ✅ 6.3.2 활성화 함수와 손실 함수

### 주요 활성화 함수

1. **시그모이드(Sigmoid)**: 출력을 0~1 사이로 압축, 주로 이진 분류에 사용
2. **하이퍼볼릭 탄젠트(tanh)**: 출력을 -1~1 사이로 압축, 중심이 0으로 시그모이드보다 학습 효율 높음
3. **ReLU(Rectified Linear Unit)**: max(0, x), 계산 효율이 좋고 그래디언트 소실 문제 완화
4. **LeakyReLU**: max(αx, x), α는 작은 상수, 음수 입력에서도 그래디언트가 0이 되지 않음
5. **Softmax**: 출력 값을 확률 분포로 변환, 다중 클래스 분류에 사용

### 주요 손실 함수

1. **평균 제곱 오차(MSE)**: 회귀 문제에 주로 사용
2. **교차 엔트로피(Cross-Entropy)**: 분류 문제에 주로 사용, 예측과 실제 확률 분포 간의 차이 측정
3. **이진 교차 엔트로피(Binary Cross-Entropy)**: 이진 분류 문제에 특화
4. **범주형 교차 엔트로피(Categorical Cross-Entropy)**: 다중 클래스 분류에 사용

```python
def activation_functions_demo():
    """다양한 활성화 함수 시각화"""
    import matplotlib.pyplot as plt
    import numpy as np
    
    # 입력 범위
    x = np.linspace(-5, 5, 100)
    
    # 활성화 함수
    sigmoid = 1 / (1 + np.exp(-x))
    tanh = np.tanh(x)
    relu = np.maximum(0, x)
    leaky_relu = np.where(x > 0, x, 0.1 * x)
    
    # 시각화
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(x, sigmoid)
    plt.title('Sigmoid')
    plt.grid(True)
    
    plt.subplot(2, 2, 2)
    plt.plot(x, tanh)
    plt.title('Tanh')
    plt.grid(True)
    
    plt.subplot(2, 2, 3)
    plt.plot(x, relu)
    plt.title('ReLU')
    plt.grid(True)
    
    plt.subplot(2, 2, 4)
    plt.plot(x, leaky_relu)
    plt.title('Leaky ReLU')
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()
```

## ✅ 6.3.3 역전파 알고리즘과 최적화

### 역전파(Backpropagation)

역전파는 신경망의 가중치를 업데이트하는 핵심 알고리즘입니다. 다음 단계로 진행됩니다:

1. **순전파(Forward Pass)**: 입력 데이터가 네트워크를 통과하여 예측 생성
2. **오차 계산**: 예측과 실제 레이블 간의 차이(손실) 계산
3. **역전파(Backward Pass)**: 손실에 대한 각 가중치의 미분(그래디언트) 계산
4. **가중치 업데이트**: 그래디언트를 사용하여 가중치 조정

### 최적화 알고리즘

1. **경사 하강법(SGD)**: 그래디언트 방향으로 파라미터 업데이트
2. **모멘텀(Momentum)**: 이전 업데이트 방향을 고려하여 지역 최소값 회피 
3. **AdaGrad**: 파라미터별 학습률 조정, 자주 업데이트되는 파라미터는 학습률 감소
4. **RMSProp**: AdaGrad의 개선 버전, 최근 그래디언트에 더 큰 가중치 부여
5. **Adam**: 모멘텀과 RMSProp의 장점을 결합한 가장 널리 사용되는 최적화 알고리즘

```python
def simple_backprop_example():
    """간단한 역전파 구현 예제"""
    import numpy as np
    
    # 간단한 2층 신경망 클래스
    class SimpleNN:
        def __init__(self, input_size, hidden_size, output_size):
            # 가중치 초기화
            self.W1 = np.random.randn(input_size, hidden_size) * 0.01
            self.b1 = np.zeros(hidden_size)
            self.W2 = np.random.randn(hidden_size, output_size) * 0.01
            self.b2 = np.zeros(output_size)
        
        def forward(self, X):
            # 순전파
            self.z1 = np.dot(X, self.W1) + self.b1
            self.a1 = self._sigmoid(self.z1)
            self.z2 = np.dot(self.a1, self.W2) + self.b2
            self.a2 = self._sigmoid(self.z2)
            return self.a2
        
        def backward(self, X, y, output, learning_rate=0.1):
            # 역전파
            m = y.shape[0]
            
            # 출력층 그래디언트
            dz2 = output - y
            dW2 = np.dot(self.a1.T, dz2) / m
            db2 = np.sum(dz2, axis=0) / m
            
            # 은닉층 그래디언트
            da1 = np.dot(dz2, self.W2.T)
            dz1 = da1 * self._sigmoid_derivative(self.z1)
            dW1 = np.dot(X.T, dz1) / m
            db1 = np.sum(dz1, axis=0) / m
            
            # 가중치 업데이트
            self.W1 -= learning_rate * dW1
            self.b1 -= learning_rate * db1
            self.W2 -= learning_rate * dW2
            self.b2 -= learning_rate * db2
        
        def train(self, X, y, epochs=1000):
            for i in range(epochs):
                # 순전파
                output = self.forward(X)
                
                # 손실 계산
                loss = self._binary_cross_entropy(y, output)
                
                # 역전파
                self.backward(X, y, output)
                
                if i % 100 == 0:
                    print(f"Epoch {i}, Loss: {loss:.4f}")
        
        def _sigmoid(self, x):
            return 1 / (1 + np.exp(-x))
        
        def _sigmoid_derivative(self, x):
            s = self._sigmoid(x)
            return s * (1 - s)
        
        def _binary_cross_entropy(self, y, y_pred):
            m = y.shape[0]
            loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) / m
            return loss
    
    # 예제 데이터 (XOR 문제)
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    # 모델 생성 및 학습
    model = SimpleNN(input_size=2, hidden_size=4, output_size=1)
    model.train(X, y, epochs=1000)
    
    # 예측
    predictions = model.forward(X)
    print("\n최종 예측:")
    for i in range(len(X)):
        print(f"입력: {X[i]}, 실제값: {y[i][0]}, 예측값: {predictions[i][0]:.4f}")
```

## ✅ 6.3.4 딥러닝 아키텍처

### 주요 신경망 구조

1. **완전 연결 신경망(Fully Connected Network)**
   - 모든 뉴런이 이전 층의 모든 뉴런과 연결된 구조
   - 응용: 분류, 회귀, 특성 추출

2. **합성곱 신경망(CNN, Convolutional Neural Network)**
   - 이미지 데이터 처리에 특화된 구조
   - 컨볼루션 층과, 풀링 층으로 구성
   - 응용: 이미지 분류, 객체 탐지, 얼굴 인식

3. **순환 신경망(RNN, Recurrent Neural Network)**
   - 시퀀스 데이터 처리를 위한 구조
   - 이전 단계의 출력이 현재 단계의 입력으로 활용
   - 응용: 자연어 처리, 시계열 예측, 음성 인식

4. **LSTM(Long Short-Term Memory)과 GRU**
   - RNN의 변형으로 장기 의존성 문제 해결
   - 특수한 게이트 메커니즘으로 정보 조절
   - 응용: 기계 번역, 감성 분석, 텍스트 생성

5. **오토인코더(Autoencoder)**
   - 비지도 학습 방식으로 데이터 압축 및 복원
   - 인코더와 디코더로 구성
   - 응용: 차원 축소, 노이즈 제거, 이상치 탐지

6. **생성적 적대 신경망(GAN, Generative Adversarial Network)**
   - 생성자와 판별자 두 네트워크의 경쟁을 통한 학습
   - 응용: 이미지 생성, 이미지 변환, 데이터 증강

### 딥러닝 모델 구현 예시

```python
def cnn_architecture():
    """CNN 아키텍처 예시"""
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
    
    # CNN 모델 구성
    model = Sequential([
        # 첫 번째 합성곱 블록
        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        MaxPooling2D((2, 2)),
        
        # 두 번째 합성곱 블록
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        
        # 세 번째 합성곱 블록
        Conv2D(64, (3, 3), activation='relu'),
        
        # 평탄화 및 완전 연결 층
        Flatten(),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(10, activation='softmax')
    ])
    
    # 모델 컴파일
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # 모델 요약 출력
    model.summary()
    
    return model
``` 