# 6.4 TensorFlow와 PyTorch 활용

## ✅ 6.4.1 TensorFlow를 사용한 신경망

TensorFlow는 Google에서 개발한 오픈소스 딥러닝 프레임워크로, 특히 Keras API를 통해 직관적인 모델 개발이 가능합니다.

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

class DeepLearningModel:
    """딥러닝 모델 클래스"""
    
    def __init__(self, input_dim, num_classes):
        self.model = Sequential([
            Dense(64, activation='relu', input_dim=input_dim),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(num_classes, activation='softmax')
        ])
        
        self.model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
    
    def train(self, X_train, y_train, epochs=10, batch_size=32):
        """모델 학습"""
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            verbose=1
        )
        return history
    
    def evaluate(self, X_test, y_test):
        """모델 평가"""
        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        print(f"테스트 손실: {loss:.4f}")
        print(f"테스트 정확도: {accuracy:.4f}")
        return loss, accuracy
```

## ✅ 6.4.2 TensorFlow의 고급 기능

TensorFlow는 다양한 고급 기능을 제공하여 복잡한 모델 구축과 학습 과정을 지원합니다.

```python
def advanced_tensorflow_features():
    """TensorFlow의 고급 기능 데모"""
    import tensorflow as tf
    import numpy as np
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, Concatenate
    
    # 1. 함수형 API를 사용한 복잡한 모델 구축
    def functional_api_example():
        """다중 입력/출력 모델 예제"""
        # 첫 번째 입력과 분기
        input_1 = Input(shape=(10,), name='input_1')
        x1 = Dense(32, activation='relu')(input_1)
        
        # 두 번째 입력과 분기
        input_2 = Input(shape=(15,), name='input_2')
        x2 = Dense(32, activation='relu')(input_2)
        
        # 두 분기 결합
        merged = Concatenate()([x1, x2])
        
        # 공통 레이어
        common = Dense(64, activation='relu')(merged)
        
        # 다중 출력
        output_1 = Dense(1, activation='sigmoid', name='binary_output')(common)
        output_2 = Dense(10, activation='softmax', name='multi_output')(common)
        
        # 모델 생성
        model = Model(inputs=[input_1, input_2], outputs=[output_1, output_2])
        
        # 컴파일
        model.compile(
            optimizer='adam',
            loss={
                'binary_output': 'binary_crossentropy',
                'multi_output': 'categorical_crossentropy'
            },
            metrics={
                'binary_output': 'accuracy',
                'multi_output': 'accuracy'
            }
        )
        
        model.summary()
        return model
    
    # 2. 커스텀 레이어 만들기
    class CustomDense(tf.keras.layers.Layer):
        """커스텀 Dense 레이어"""
        def __init__(self, units, activation=None):
            super(CustomDense, self).__init__()
            self.units = units
            self.activation = tf.keras.activations.get(activation)
        
        def build(self, input_shape):
            self.w = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='random_normal',
                trainable=True
            )
            self.b = self.add_weight(
                shape=(self.units,),
                initializer='zeros',
                trainable=True
            )
        
        def call(self, inputs):
            output = tf.matmul(inputs, self.w) + self.b
            if self.activation is not None:
                output = self.activation(output)
            return output
    
    # 3. 커스텀 손실 함수
    def custom_loss_function():
        """가중치가 있는 MSE 손실 함수"""
        def weighted_mse(y_true, y_pred):
            # 오차가 큰 샘플에 더 높은 가중치 부여
            squared_error = tf.square(y_true - y_pred)
            weighted_squared_error = squared_error * (1 + tf.abs(y_true))
            return tf.reduce_mean(weighted_squared_error)
        
        return weighted_mse
    
    # 4. TensorFlow 데이터 파이프라인
    def tf_data_pipeline():
        """효율적인 데이터 파이프라인 예제"""
        # 대용량 데이터셋 시뮬레이션
        def generate_data():
            for i in range(1000):
                x = np.random.normal(0, 1, (100,))
                y = np.random.randint(0, 5, (1,))
                yield (x, y)
        
        # 데이터셋 생성
        dataset = tf.data.Dataset.from_generator(
            generate_data,
            output_signature=(
                tf.TensorSpec(shape=(100,), dtype=tf.float32),
                tf.TensorSpec(shape=(1,), dtype=tf.int32)
            )
        )
        
        # 데이터셋 변환과 최적화
        dataset = dataset.batch(32)
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        dataset = dataset.cache()
        
        return dataset
    
    # 함수형 API 예제 실행
    model = functional_api_example()
    
    # 데이터 파이프라인 생성
    dataset = tf_data_pipeline()
    
    print("\nTensorFlow의 고급 기능이 성공적으로 데모되었습니다.")
    return model, dataset
```

## ✅ 6.4.3 PyTorch 기초

PyTorch는 Facebook에서 개발한 딥러닝 프레임워크로, 동적 계산 그래프와 직관적인 API가 특징입니다.

```python
def pytorch_basics():
    """PyTorch 기초 예제"""
    try:
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        # 간단한 PyTorch 모델 클래스
        class SimpleNN(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super(SimpleNN, self).__init__()
                # 레이어 정의
                self.fc1 = nn.Linear(input_dim, hidden_dim)
                self.relu = nn.ReLU()
                self.fc2 = nn.Linear(hidden_dim, output_dim)
                self.softmax = nn.Softmax(dim=1)
                
            def forward(self, x):
                # 순전파 정의
                x = self.fc1(x)
                x = self.relu(x)
                x = self.fc2(x)
                x = self.softmax(x)
                return x
        
        # 모델 초기화
        input_dim = 10
        hidden_dim = 20
        output_dim = 5
        model = SimpleNN(input_dim, hidden_dim, output_dim)
        
        # 손실 함수와 옵티마이저 정의
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        # 모의 데이터 생성
        batch_size = 32
        x = torch.randn(batch_size, input_dim)
        y = torch.randint(0, output_dim, (batch_size,))
        
        # 학습 루프 (일반적인 PyTorch 학습 패턴)
        def train_step():
            # 그래디언트 초기화
            optimizer.zero_grad()
            
            # 순전파
            outputs = model(x)
            
            # 손실 계산
            loss = criterion(outputs, y)
            
            # 역전파
            loss.backward()
            
            # 가중치 업데이트
            optimizer.step()
            
            return loss.item()
        
        # 학습 스텝 실행
        loss = train_step()
        print(f"학습 손실: {loss:.4f}")
        
        # 예측
        with torch.no_grad():
            test_input = torch.randn(1, input_dim)
            prediction = model(test_input)
            print(f"예측 확률: {prediction[0]}")
            predicted_class = torch.argmax(prediction, dim=1).item()
            print(f"예측 클래스: {predicted_class}")
        
        return model
        
    except ImportError:
        print("PyTorch가 설치되어 있지 않습니다. 'pip install torch'로 설치하세요.")
        return None
```

## ✅ 6.4.4 PyTorch 고급 기능

PyTorch의 고급 기능과 커스터마이징 방법을 살펴봅니다.

```python
def advanced_pytorch_features():
    """PyTorch의 고급 기능 데모"""
    try:
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import torchvision
        import numpy as np
        
        # 1. 커스텀 데이터셋 클래스
        class CustomDataset(torch.utils.data.Dataset):
            """커스텀 데이터셋 클래스"""
            def __init__(self, num_samples=1000):
                self.data = torch.randn(num_samples, 10)
                self.targets = torch.randint(0, 5, (num_samples,))
                
            def __len__(self):
                return len(self.data)
                
            def __getitem__(self, idx):
                return self.data[idx], self.targets[idx]
        
        # 데이터셋과 데이터로더 생성
        dataset = CustomDataset()
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=32, shuffle=True, num_workers=0
        )
        
        # 2. 커스텀 레이어 정의
        class AttentionLayer(nn.Module):
            """단순한 셀프 어텐션 레이어"""
            def __init__(self, input_dim):
                super(AttentionLayer, self).__init__()
                self.query = nn.Linear(input_dim, input_dim)
                self.key = nn.Linear(input_dim, input_dim)
                self.value = nn.Linear(input_dim, input_dim)
                
            def forward(self, x):
                # x: [batch_size, seq_len, input_dim]
                q = self.query(x)
                k = self.key(x)
                v = self.value(x)
                
                # 어텐션 점수 계산
                scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))
                
                # 소프트맥스로 가중치 정규화
                attention_weights = F.softmax(scores, dim=-1)
                
                # 가중 합계 계산
                output = torch.matmul(attention_weights, v)
                
                return output
        
        # 3. 전이 학습 예제
        def transfer_learning_example():
            """사전 훈련된 모델 활용"""
            # ResNet-18 로드
            model = torchvision.models.resnet18(pretrained=True)
            
            # 특성 추출을 위해 가중치 고정
            for param in model.parameters():
                param.requires_grad = False
                
            # 새로운 분류 계층 추가
            num_features = model.fc.in_features
            model.fc = nn.Linear(num_features, 10)  # 10개 클래스로 변경
            
            # 새 계층의 가중치만 학습
            optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)
            
            return model
        
        # 4. 체크포인트 저장 및 로드
        def checkpoint_example(model):
            """모델 체크포인트 저장 및 로드"""
            # 체크포인트 저장
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': 10,
                'loss': 0.123
            }
            
            torch.save(checkpoint, 'model_checkpoint.pth')
            
            # 체크포인트 불러오기
            checkpoint = torch.load('model_checkpoint.pth')
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            epoch = checkpoint['epoch']
            loss = checkpoint['loss']
            
            print(f"체크포인트에서 복원: 에포크 {epoch}, 손실 {loss:.4f}")
        
        # 5. 분산 학습 예제
        def distributed_training():
            """PyTorch 분산 학습 설정"""
            if torch.cuda.device_count() > 1:
                print(f"GPU {torch.cuda.device_count()}개로 분산 학습")
                model = nn.DataParallel(model)
            
            return model
        
        # 모의 학습 루프 정의
        def mock_training():
            # 모델과 옵티마이저 정의
            model = nn.Sequential(
                nn.Linear(10, 20),
                nn.ReLU(),
                nn.Linear(20, 5)
            )
            
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            # 간단한 학습 루프
            num_epochs = 2
            for epoch in range(num_epochs):
                for batch_idx, (data, target) in enumerate(dataloader):
                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    loss.backward()
                    optimizer.step()
                    
                    if batch_idx % 10 == 0:
                        print(f"에포크 {epoch}, 배치 {batch_idx}, 손실: {loss.item():.4f}")
                        break  # 간단한 데모를 위해 일부만 실행
            
            return model, optimizer
            
        # 데모 실행
        model, optimizer = mock_training()
        
        print("\nPyTorch의 고급 기능이 성공적으로 데모되었습니다.")
        return model
        
    except ImportError:
        print("PyTorch 또는 torchvision이 설치되어 있지 않습니다.")
        print("'pip install torch torchvision'으로 설치하세요.")
        return None
```

## ✅ 6.4.5 TensorFlow와 PyTorch 비교

두 프레임워크의 주요 차이점과 각각의 장단점을 비교합니다.

| 특성 | TensorFlow (with Keras) | PyTorch |
|------|--------------------------|---------|
| **계산 그래프** | 정적 그래프 (Eager Execution으로 동적도 가능) | 동적 그래프 |
| **문법** | 선언적 프로그래밍 스타일 | 명령형 프로그래밍 스타일 |
| **디버깅** | 복잡함 (Eager Execution으로 개선) | 쉬움 (파이썬 네이티브 디버깅) |
| **배포** | 강력한 프로덕션 도구 (TF Serving, TF Lite) | 비교적 제한적 (TorchScript, TorchServe) |
| **성능** | 대규모 모델 및 분산 학습에 최적화 | 연구 및 실험에 최적화 |
| **커뮤니티** | 대규모, 산업 중심 | 빠르게 성장 중, 연구 중심 |
| **주요 사용자** | Google, 대기업, 모바일/엣지 배포 | Facebook, 학계, 연구소 |
| **특화 분야** | 프로덕션 배포, 대규모 훈련, 모바일 | 연구, 빠른 실험, 자연어 처리 |

```python
def framework_choice_guide():
    """프레임워크 선택 가이드"""
    print("TensorFlow를 선택해야 하는 경우:")
    print("- 프로덕션 환경에 모델을 배포해야 할 때")
    print("- 모바일 또는 엣지 디바이스에서 추론해야 할 때")
    print("- TensorBoard와 같은 강력한 시각화 도구가 필요할 때")
    print("- 대규모 분산 학습이 필요할 때")
    
    print("\nPyTorch를 선택해야 하는 경우:")
    print("- 연구 프로젝트를 진행할 때")
    print("- 빠른 프로토타이핑과 실험이 필요할 때")
    print("- 복잡한 모델 아키텍처를 만들고 디버깅해야 할 때")
    print("- 파이썬 생태계와의 자연스러운 통합이 중요할 때")
    
    print("\n실용적인 조언: 두 프레임워크 모두 강력하고 성숙하므로,")
    print("프로젝트 요구사항과 팀의 친숙도에 따라 선택하는 것이 좋습니다.") 