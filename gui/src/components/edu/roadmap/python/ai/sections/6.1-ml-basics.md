# 6.1 머신러닝 개념 및 기본 원리

## ✅ 6.1.1 머신러닝의 종류
1. **지도 학습**
   - 분류(Classification)
   - 회귀(Regression)
2. **비지도 학습**
   - 군집화(Clustering)
   - 차원 축소(Dimensionality Reduction)
3. **강화 학습**
   - 보상 기반 학습
   - 환경과 상호작용

## ✅ 6.1.2 머신러닝 프로세스
1. 데이터 수집 및 전처리
2. 모델 선택 및 학습
3. 평가 및 최적화
4. 예측 및 배포

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

class MLProject:
    """머신러닝 프로젝트 기본 클래스"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        
    def prepare_data(self, X, y, test_size=0.2):
        """데이터 전처리 및 분할"""
        # 데이터 분할
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        
        # 특성 스케일링
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test
```

## ✅ 6.1.3 지도 학습의 이해

지도 학습은 입력(X)과 출력(y) 쌍으로 구성된 레이블이 있는 데이터를 사용하여 함수를 학습하는 방법입니다. 주요 유형으로는 분류와 회귀가 있습니다.

1. **분류(Classification)**
   - 목표: 입력 데이터를 미리 정의된 클래스로 분류
   - 예시: 이메일 스팸 필터링, 손글씨 인식, 질병 진단
   - 알고리즘: 로지스틱 회귀, 결정 트리, 랜덤 포레스트, SVM

2. **회귀(Regression)**
   - 목표: 연속적인 출력값 예측
   - 예시: 주택 가격 예측, 판매량 예측, 온도 예측
   - 알고리즘: 선형 회귀, 다항 회귀, 결정 트리 회귀, 신경망

## ✅ 6.1.4 비지도 학습과 강화 학습

1. **비지도 학습(Unsupervised Learning)**
   - 레이블이 없는 데이터에서 패턴 발견
   - **군집화(Clustering)**: 유사한 데이터 포인트를 그룹화 (예: K-means, DBSCAN)
   - **차원 축소(Dimensionality Reduction)**: 복잡한 데이터의 중요 특성 추출 (예: PCA, t-SNE)
   - **이상치 탐지(Anomaly Detection)**: 일반적인 패턴에서 벗어난 데이터 식별

2. **강화 학습(Reinforcement Learning)**
   - 에이전트가 환경과 상호작용하며 보상 최대화를 학습
   - 구성요소: 상태(State), 행동(Action), 보상(Reward), 정책(Policy)
   - 예시: 로봇 제어, 게임 플레이, 자율 주행
   - 알고리즘: Q-Learning, Deep Q Networks (DQN), Proximal Policy Optimization (PPO)

## ✅ 6.1.5 과적합과 일반화

모델 학습의 핵심 목표는 새로운 데이터에 대한 일반화 능력을 향상시키는 것입니다.

- **과적합(Overfitting)**
  - 학습 데이터에 지나치게 최적화되어 일반화 성능이 저하됨
  - 해결책: 규제화(Regularization), 드롭아웃(Dropout), 교차 검증(Cross-validation)

- **과소적합(Underfitting)**
  - 모델이 데이터의 패턴을 충분히 학습하지 못함
  - 해결책: 모델 복잡도 증가, 특성 추가, 하이퍼파라미터 최적화

- **편향-분산 트레이드오프(Bias-Variance Tradeoff)**
  - 편향(Bias): 모델의 가정으로 인한 오차
  - 분산(Variance): 학습 데이터에 따른 모델 출력의 변동성
  - 최적의 모델은 편향과 분산 사이의 균형을 맞춤 