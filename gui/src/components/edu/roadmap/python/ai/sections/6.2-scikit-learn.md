# 6.2 scikit-learn을 사용한 머신러닝

## ✅ 6.2.1 분류 모델 구현

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

class ClassificationModel(MLProject):
    """분류 모델 클래스"""
    
    def __init__(self):
        super().__init__()
        self.model = RandomForestClassifier(
            n_estimators=100,
            random_state=42
        )
    
    def train(self, X_train, y_train):
        """모델 학습"""
        self.model.fit(X_train, y_train)
    
    def evaluate(self, X_test, y_test):
        """모델 평가"""
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        
        print(f"모델 정확도: {accuracy:.4f}")
        print("\n분류 보고서:")
        print(report)
        
        return accuracy, report
```

## ✅ 6.2.2 회귀 모델 구현

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

class RegressionModel(MLProject):
    """회귀 모델 클래스"""
    
    def __init__(self):
        super().__init__()
        self.model = LinearRegression()
    
    def train(self, X_train, y_train):
        """모델 학습"""
        self.model.fit(X_train, y_train)
    
    def evaluate(self, X_test, y_test):
        """모델 평가"""
        y_pred = self.model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"평균 제곱 오차: {mse:.4f}")
        print(f"R² 점수: {r2:.4f}")
        
        return mse, r2
```

## ✅ 6.2.3 주요 알고리즘과 활용 사례

### 분류 알고리즘
1. **로지스틱 회귀(Logistic Regression)**
   - 선형 모델에 시그모이드 함수를 적용한 이진 분류 알고리즘
   - 활용: 이진 분류, 다중 클래스 분류, 확률 계산이 필요한 경우

2. **결정 트리(Decision Tree)**
   - 특성을 기반으로 질문을 분기하여 결정을 내리는 트리 구조
   - 활용: 범주형 및 연속형 데이터, 의사 결정 규칙 시각화

3. **랜덤 포레스트(Random Forest)**
   - 여러 결정 트리의 앙상블 방법으로 과적합 방지
   - 활용: 고차원 데이터, 특성 중요도 파악

4. **서포트 벡터 머신(SVM)**
   - 클래스 간 최대 마진을 가진 결정 경계를 찾는 알고리즘
   - 활용: 텍스트 분류, 이미지 분류, 비선형 문제

### 회귀 알고리즘
1. **선형 회귀(Linear Regression)**
   - 종속 변수와 독립 변수 간의 선형 관계 모델링
   - 활용: 주택 가격 예측, 판매량 예측, 관계 분석

2. **릿지 회귀(Ridge Regression)**
   - L2 규제화를 적용한 선형 회귀로 과적합 방지
   - 활용: 다중 공선성이 있는 데이터셋

3. **라쏘 회귀(Lasso Regression)**
   - L1 규제화를 적용한 선형 회귀로 특성 선택 효과
   - 활용: 특성이 많고 일부만 중요한 경우

### 비지도 학습 알고리즘
1. **K-평균 군집화(K-means Clustering)**
   - 데이터를 K개의 클러스터로 그룹화하는 알고리즘
   - 활용: 고객 세분화, 이미지 압축, 이상치 탐지

2. **주성분 분석(PCA)**
   - 고차원 데이터의 분산을 최대화하는 방향으로 차원 축소
   - 활용: 데이터 시각화, 차원 축소, 특성 추출

## ✅ 6.2.4 모델 선택 및 하이퍼파라미터 최적화

```python
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier

def optimize_model():
    """하이퍼파라미터 최적화 예제"""
    
    # 데이터 생성
    from sklearn.datasets import load_iris
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # 모델 정의
    model = RandomForestClassifier(random_state=42)
    
    # 파라미터 그리드 정의
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    # 교차 검증으로 그리드 서치
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5,
        scoring='accuracy',
        n_jobs=-1
    )
    grid_search.fit(X, y)
    
    # 결과 확인
    print(f"최적 파라미터: {grid_search.best_params_}")
    print(f"최고 정확도: {grid_search.best_score_:.4f}")
    
    # 최적 모델로 교차 검증
    best_model = grid_search.best_estimator_
    cv_scores = cross_val_score(best_model, X, y, cv=5)
    print(f"교차 검증 점수: {cv_scores}")
    print(f"평균 정확도: {cv_scores.mean():.4f}")
    
    return best_model

# 모델 최적화 실행
# optimize_model()
```

## ✅ 6.2.5 특성 엔지니어링과 전처리

특성 엔지니어링은 모델의 성능을 크게 향상시킬 수 있는 중요한 단계입니다.

```python
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

def feature_engineering_pipeline():
    """특성 엔지니어링 파이프라인 예제"""
    
    # 수치형 특성 전처리
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # 범주형 특성 전처리
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # 전처리 과정 조합
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, ['age', 'income', 'score']),
            ('cat', categorical_transformer, ['gender', 'country', 'category'])
        ]
    )
    
    # 전체 파이프라인 구성 (전처리 + 모델)
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier())
    ])
    
    # 실제 사용법:
    # pipeline.fit(X_train, y_train)
    # y_pred = pipeline.predict(X_test)
    
    return pipeline
``` 