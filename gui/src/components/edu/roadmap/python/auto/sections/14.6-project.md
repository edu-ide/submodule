
# 14.6 자동화 실습 프로젝트

자동화 기술을 활용한 실제 프로젝트를 통해 지금까지 배운 개념들을 통합적으로 적용해 보겠습니다. 이 섹션에서는 파일 관리, 이메일 발송, 웹 자동화, 스케줄링 등을 결합한 종합적인 프로젝트를 개발합니다.

## ✅ 14.6.1 파일 모니터링 및 처리 시스템

이 프로젝트에서는 특정 디렉토리를 모니터링하고, 새로 추가된 파일을 자동으로 처리하는 시스템을 구축합니다.

```python
import os
import time
import shutil
import logging
import json
import csv
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
from datetime import datetime, timedelta
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import pandas as pd
import schedule
import threading

# 로깅 설정
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("file_monitor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("file_monitor")

class FileMonitoringSystem:
    """파일 모니터링 및 처리 시스템"""
    
    def __init__(self, config_path="config.json"):
        """
        시스템 초기화
        config_path: 설정 파일 경로
        """
        self.load_config(config_path)
        self.setup_directories()
        self.observer = None
        self.running = False
        self.statistics = {
            "processed_files": 0,
            "failed_files": 0,
            "last_processed_time": None,
            "processed_by_type": {}
        }
    
    def load_config(self, config_path):
        """설정 파일 로드"""
        try:
            with open(config_path, 'r') as f:
                self.config = json.load(f)
            logger.info(f"설정 파일 로드 완료: {config_path}")
        except FileNotFoundError:
            logger.warning(f"설정 파일을 찾을 수 없음: {config_path}, 기본 설정 사용")
            self.config = {
                "watch_directory": "./watched",
                "processed_directory": "./processed",
                "failed_directory": "./failed",
                "archive_directory": "./archive",
                "backup_directory": "./backup",
                "email_config": {
                    "enabled": False,
                    "smtp_server": "smtp.example.com",
                    "smtp_port": 587,
                    "username": "user@example.com",
                    "password": "password",
                    "from_address": "user@example.com",
                    "to_address": "admin@example.com"
                },
                "file_types": {
                    "csv": {
                        "processor": "csv_processor",
                        "destination": "data"
                    },
                    "txt": {
                        "processor": "text_processor",
                        "destination": "documents"
                    },
                    "jpg": {
                        "processor": "image_processor",
                        "destination": "images"
                    },
                    "png": {
                        "processor": "image_processor",
                        "destination": "images"
                    },
                    "pdf": {
                        "processor": "document_processor",
                        "destination": "documents"
                    }
                },
                "backup_schedule": {
                    "enabled": True,
                    "interval_hours": 24,
                    "retention_days": 7
                },
                "report_schedule": {
                    "enabled": True,
                    "interval_hours": 24,
                    "recipients": ["admin@example.com"]
                }
            }
            # 기본 설정 저장
            os.makedirs(os.path.dirname(config_path), exist_ok=True)
            with open(config_path, 'w') as f:
                json.dump(self.config, f, indent=4)
            logger.info(f"기본 설정 파일 생성: {config_path}")
    
    def setup_directories(self):
        """필요한 디렉토리 생성"""
        directories = [
            self.config["watch_directory"],
            self.config["processed_directory"],
            self.config["failed_directory"],
            self.config["archive_directory"],
            self.config["backup_directory"]
        ]
        
        # 파일 유형별 목적지 디렉토리 추가
        for file_type, settings in self.config["file_types"].items():
            if "destination" in settings:
                dest_path = os.path.join(self.config["processed_directory"], settings["destination"])
                directories.append(dest_path)
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"디렉토리 생성: {directory}")
    
    def start_monitoring(self):
        """파일 모니터링 시작"""
        if self.running:
            logger.warning("이미 모니터링 중입니다.")
            return
        
        self.running = True
        
        # watchdog 이벤트 핸들러 설정
        event_handler = FileEventHandler(self)
        self.observer = Observer()
        self.observer.schedule(event_handler, self.config["watch_directory"], recursive=False)
        self.observer.start()
        
        logger.info(f"디렉토리 모니터링 시작: {self.config['watch_directory']}")
        
        # 스케줄러 시작
        self.start_schedulers()
    
    def stop_monitoring(self):
        """파일 모니터링 중지"""
        if not self.running:
            logger.warning("모니터링 중이 아닙니다.")
            return
        
        self.running = False
        
        if self.observer:
            self.observer.stop()
            self.observer.join()
        
        logger.info("디렉토리 모니터링 중지")
    
    def start_schedulers(self):
        """스케줄링된 작업 시작"""
        # 백업 스케줄 설정
        if self.config["backup_schedule"]["enabled"]:
            interval = self.config["backup_schedule"]["interval_hours"]
            schedule.every(interval).hours.do(self.create_backup)
            logger.info(f"백업 스케줄 설정됨: {interval}시간마다")
        
        # 보고서 스케줄 설정
        if self.config["report_schedule"]["enabled"]:
            interval = self.config["report_schedule"]["interval_hours"]
            schedule.every(interval).hours.do(self.generate_and_send_report)
            logger.info(f"보고서 스케줄 설정됨: {interval}시간마다")
        
        # 오래된 백업 정리 스케줄 설정
        schedule.every().day.at("01:00").do(self.cleanup_old_backups)
        
        # 스케줄러 스레드 시작
        scheduler_thread = threading.Thread(target=self._run_scheduler)
        scheduler_thread.daemon = True
        scheduler_thread.start()
    
    def _run_scheduler(self):
        """스케줄러 실행 루프"""
        while self.running:
            schedule.run_pending()
            time.sleep(1)
    
    def process_file(self, file_path):
        """파일 처리"""
        try:
            filename = os.path.basename(file_path)
            file_ext = os.path.splitext(filename)[1][1:].lower()
            
            # 파일 유형 확인
            if file_ext not in self.config["file_types"]:
                logger.warning(f"지원되지 않는 파일 유형: {file_ext} - {filename}")
                self._move_to_failed(file_path, "지원되지 않는 파일 유형")
                return False
            
            # 파일 처리 함수 결정
            processor_name = self.config["file_types"][file_ext]["processor"]
            processor = getattr(self, processor_name, None)
            
            if not processor:
                logger.error(f"파일 처리기를 찾을 수 없음: {processor_name}")
                self._move_to_failed(file_path, "처리기를 찾을 수 없음")
                return False
            
            # 파일 처리
            logger.info(f"파일 처리 시작: {filename}")
            result = processor(file_path)
            
            # 처리 결과에 따라 파일 이동
            if result:
                # 파일 유형별 목적지 디렉토리로 이동
                destination = os.path.join(
                    self.config["processed_directory"],
                    self.config["file_types"][file_ext]["destination"]
                )
                
                # 파일 이름 충돌 방지를 위한 타임스탬프 추가
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                new_filename = f"{os.path.splitext(filename)[0]}_{timestamp}{os.path.splitext(filename)[1]}"
                dest_path = os.path.join(destination, new_filename)
                
                shutil.move(file_path, dest_path)
                logger.info(f"파일 처리 완료 및 이동됨: {filename} -> {dest_path}")
                
                # 통계 업데이트
                self.statistics["processed_files"] += 1
                self.statistics["last_processed_time"] = datetime.now().isoformat()
                
                file_type = file_ext
                if file_type not in self.statistics["processed_by_type"]:
                    self.statistics["processed_by_type"][file_type] = 0
                self.statistics["processed_by_type"][file_type] += 1
                
                return True
            else:
                self._move_to_failed(file_path, "처리 실패")
                return False
                
        except Exception as e:
            logger.error(f"파일 처리 중 오류 발생: {e}", exc_info=True)
            self._move_to_failed(file_path, str(e))
            return False
    
    def _move_to_failed(self, file_path, reason):
        """실패한 파일 이동"""
        try:
            filename = os.path.basename(file_path)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            new_filename = f"{os.path.splitext(filename)[0]}_{timestamp}{os.path.splitext(filename)[1]}"
            dest_path = os.path.join(self.config["failed_directory"], new_filename)
            
            # 실패 이유 로그 파일 생성
            log_file = os.path.splitext(dest_path)[0] + ".log"
            with open(log_file, 'w') as f:
                f.write(f"실패 시간: {datetime.now().isoformat()}\n")
                f.write(f"파일: {filename}\n")
                f.write(f"이유: {reason}\n")
            
            # 파일 이동
            shutil.move(file_path, dest_path)
            logger.warning(f"파일을 실패 디렉토리로 이동: {filename} -> {dest_path}")
            
            # 통계 업데이트
            self.statistics["failed_files"] += 1
            
        except Exception as e:
            logger.error(f"실패 파일 이동 중 오류 발생: {e}", exc_info=True)
    
    def csv_processor(self, file_path):
        """CSV 파일 처리"""
        try:
            # CSV 파일 검증 및 정규화
            df = pd.read_csv(file_path)
            
            # 기본 검증 (예: 빈 파일, 필수 열 존재 여부 등)
            if df.empty:
                logger.warning(f"빈 CSV 파일: {file_path}")
                return False
            
            # 정규화된 CSV 파일 저장 (원본은 이후에 이동됨)
            normalized_path = file_path + ".normalized.csv"
            df.to_csv(normalized_path, index=False)
            
            logger.info(f"CSV 파일 처리됨: {file_path}, 행 수: {len(df)}")
            return True
            
        except Exception as e:
            logger.error(f"CSV 처리 중 오류 발생: {e}", exc_info=True)
            return False
    
    def text_processor(self, file_path):
        """텍스트 파일 처리"""
        try:
            # 텍스트 파일 읽기 및 기본 검증
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # 기본 검증 (예: 빈 파일 여부)
            if not content.strip():
                logger.warning(f"빈 텍스트 파일: {file_path}")
                return False
            
            # 텍스트 처리 (예: 정규화, 포맷팅 등)
            word_count = len(content.split())
            
            # 처리 결과 저장
            stats_path = file_path + ".stats.txt"
            with open(stats_path, 'w', encoding='utf-8') as f:
                f.write(f"파일명: {os.path.basename(file_path)}\n")
                f.write(f"분석 시간: {datetime.now().isoformat()}\n")
                f.write(f"단어 수: {word_count}\n")
                f.write(f"문자 수: {len(content)}\n")
                f.write(f"라인 수: {len(content.splitlines())}\n")
            
            logger.info(f"텍스트 파일 처리됨: {file_path}, 단어 수: {word_count}")
            return True
            
        except Exception as e:
            logger.error(f"텍스트 처리 중 오류 발생: {e}", exc_info=True)
            return False
    
    def image_processor(self, file_path):
        """이미지 파일 처리 (메타데이터 추출)"""
        try:
            # 이 예제에서는 간단한 파일 정보만 추출
            file_info = os.stat(file_path)
            file_size = file_info.st_size
            
            # 메타데이터 저장
            metadata_path = file_path + ".metadata.txt"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                f.write(f"파일명: {os.path.basename(file_path)}\n")
                f.write(f"분석 시간: {datetime.now().isoformat()}\n")
                f.write(f"파일 크기: {file_size} 바이트\n")
                f.write(f"생성 시간: {datetime.fromtimestamp(file_info.st_ctime).isoformat()}\n")
                f.write(f"수정 시간: {datetime.fromtimestamp(file_info.st_mtime).isoformat()}\n")
            
            logger.info(f"이미지 파일 처리됨: {file_path}, 크기: {file_size} 바이트")
            return True
            
        except Exception as e:
            logger.error(f"이미지 처리 중 오류 발생: {e}", exc_info=True)
            return False
    
    def document_processor(self, file_path):
        """문서 파일 처리"""
        try:
            # 이 예제에서는 간단한 파일 정보만 추출
            file_info = os.stat(file_path)
            file_size = file_info.st_size
            
            # 메타데이터 저장
            metadata_path = file_path + ".metadata.txt"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                f.write(f"파일명: {os.path.basename(file_path)}\n")
                f.write(f"분석 시간: {datetime.now().isoformat()}\n")
                f.write(f"파일 크기: {file_size} 바이트\n")
                f.write(f"생성 시간: {datetime.fromtimestamp(file_info.st_ctime).isoformat()}\n")
                f.write(f"수정 시간: {datetime.fromtimestamp(file_info.st_mtime).isoformat()}\n")
            
            logger.info(f"문서 파일 처리됨: {file_path}, 크기: {file_size} 바이트")
            return True
            
        except Exception as e:
            logger.error(f"문서 처리 중 오류 발생: {e}", exc_info=True)
            return False
    
    def create_backup(self):
        """처리된 파일 백업 생성"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = os.path.join(self.config["backup_directory"], f"backup_{timestamp}")
            os.makedirs(backup_dir, exist_ok=True)
            
            # 처리된 파일 백업
            processed_dir = self.config["processed_directory"]
            for root, _, files in os.walk(processed_dir):
                for file in files:
                    # 메타데이터 파일 제외
                    if file.endswith(".metadata.txt") or file.endswith(".stats.txt") or file.endswith(".normalized.csv"):
                        continue
                    
                    # 원본 파일 경로
                    src_path = os.path.join(root, file)
                    
                    # 상대 경로 계산
                    rel_path = os.path.relpath(root, processed_dir)
                    
                    # 대상 디렉토리 경로
                    dest_dir = os.path.join(backup_dir, rel_path)
                    os.makedirs(dest_dir, exist_ok=True)
                    
                    # 파일 복사
                    shutil.copy2(src_path, os.path.join(dest_dir, file))
            
            logger.info(f"백업 생성 완료: {backup_dir}")
            return backup_dir
            
        except Exception as e:
            logger.error(f"백업 생성 중 오류 발생: {e}", exc_info=True)
            return None
    
    def cleanup_old_backups(self):
        """오래된 백업 정리"""
        try:
            retention_days = self.config["backup_schedule"]["retention_days"]
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            # 백업 디렉토리 순회
            for item in os.listdir(self.config["backup_directory"]):
                if item.startswith("backup_"):
                    backup_path = os.path.join(self.config["backup_directory"], item)
                    
                    # 디렉토리인 경우에만 처리
                    if os.path.isdir(backup_path):
                        # 타임스탬프 추출 (backup_YYYYMMDD_HHMMSS 형식)
                        try:
                            timestamp_str = item[7:]  # "backup_" 제거
                            backup_date = datetime.strptime(timestamp_str, "%Y%m%d_%H%M%S")
                            
                            # 오래된 백업 삭제
                            if backup_date < cutoff_date:
                                shutil.rmtree(backup_path)
                                logger.info(f"오래된 백업 삭제됨: {item}")
                        except ValueError:
                            logger.warning(f"백업 디렉토리 이름 형식 오류: {item}")
            
            logger.info(f"{retention_days}일보다 오래된 백업 정리 완료")
            
        except Exception as e:
            logger.error(f"백업 정리 중 오류 발생: {e}", exc_info=True)
    
    def generate_and_send_report(self):
        """처리 보고서 생성 및 발송"""
        try:
            # 보고서 생성
            report = self.generate_report()
            
            # 이메일 발송 설정이 되어 있고 활성화된 경우에만 발송
            if self.config["email_config"]["enabled"]:
                self.send_email_report(report)
            
            # 보고서 파일로 저장
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = os.path.join(self.config["archive_directory"], f"report_{timestamp}.txt")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"보고서 생성 완료: {report_path}")
            return report_path
            
        except Exception as e:
            logger.error(f"보고서 생성 중 오류 발생: {e}", exc_info=True)
            return None
    
    def generate_report(self):
        """처리 통계 보고서 생성"""
        now = datetime.now()
        report = f"파일 처리 시스템 보고서 - {now.strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += "=" * 50 + "\n\n"
        
        # 기본 통계
        report += "기본 통계:\n"
        report += f"- 처리된 파일 수: {self.statistics['processed_files']}\n"
        report += f"- 실패한 파일 수: {self.statistics['failed_files']}\n"
        
        if self.statistics["last_processed_time"]:
            last_time = datetime.fromisoformat(self.statistics["last_processed_time"])
            report += f"- 마지막 처리 시간: {last_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        else:
            report += "- 마지막 처리 시간: 없음\n"
        
        report += "\n"
        
        # 파일 유형별 통계
        report += "파일 유형별 통계:\n"
        for file_type, count in self.statistics["processed_by_type"].items():
            report += f"- {file_type}: {count}개\n"
        
        if not self.statistics["processed_by_type"]:
            report += "- 처리된 파일 없음\n"
        
        report += "\n"
        
        # 디렉토리 사용량
        report += "디렉토리 사용량:\n"
        for dir_name, dir_path in [
            ("감시 디렉토리", self.config["watch_directory"]),
            ("처리됨 디렉토리", self.config["processed_directory"]),
            ("실패 디렉토리", self.config["failed_directory"]),
            ("백업 디렉토리", self.config["backup_directory"])
        ]:
            size = self._get_dir_size(dir_path)
            report += f"- {dir_name}: {self._format_size(size)}\n"
        
        report += "\n"
        
        # 최근 실패 목록
        report += "최근 실패 파일:\n"
        failed_files = self._get_recent_failed_files(5)
        for file_info in failed_files:
            report += f"- {file_info['filename']} - {file_info['reason']} ({file_info['time']})\n"
        
        if not failed_files:
            report += "- 최근 실패 파일 없음\n"
        
        return report
    
    def _get_dir_size(self, dir_path):
        """디렉토리 크기 계산"""
        total_size = 0
        for dirpath, _, filenames in os.walk(dir_path):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                if os.path.exists(fp):
                    total_size += os.path.getsize(fp)
        return total_size
    
    def _format_size(self, size_bytes):
        """바이트 크기를 사람이 읽기 쉬운 형식으로 변환"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} PB"
    
    def _get_recent_failed_files(self, limit=5):
        """최근 실패한 파일 목록 가져오기"""
        failed_files = []
        
        try:
            # 실패 디렉토리 내의 로그 파일 찾기
            for filename in os.listdir(self.config["failed_directory"]):
                if filename.endswith(".log"):
                    log_path = os.path.join(self.config["failed_directory"], filename)
                    with open(log_path, 'r') as f:
                        lines = f.readlines()
                    
                    # 기본 정보 추출
                    time_line = lines[0] if lines else ""
                    file_line = lines[1] if len(lines) > 1 else ""
                    reason_line = lines[2] if len(lines) > 2 else ""
                    
                    # 정보 파싱
                    time_str = time_line.split(":", 1)[1].strip() if ":" in time_line else ""
                    file_name = file_line.split(":", 1)[1].strip() if ":" in file_line else ""
                    reason = reason_line.split(":", 1)[1].strip() if ":" in reason_line else ""
                    
                    # 타임스탬프 파싱
                    try:
                        dt = datetime.fromisoformat(time_str)
                        time_formatted = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except ValueError:
                        time_formatted = time_str
                    
                    # 결과 추가
                    failed_files.append({
                        "filename": file_name,
                        "time": time_formatted,
                        "reason": reason
                    })
            
            # 시간순 정렬 및 제한
            failed_files.sort(key=lambda x: x["time"], reverse=True)
            return failed_files[:limit]
            
        except Exception as e:
            logger.error(f"실패 파일 목록 가져오기 중 오류 발생: {e}", exc_info=True)
            return []
    
    def send_email_report(self, report_content):
        """이메일로 보고서 발송"""
        try:
            email_config = self.config["email_config"]
            
            # 이메일 구성
            msg = MIMEMultipart()
            msg['From'] = email_config["from_address"]
            msg['To'] = ", ".join(self.config["report_schedule"]["recipients"])
            msg['Subject'] = f"파일 처리 시스템 보고서 - {datetime.now().strftime('%Y-%m-%d')}"
            
            # 본문 추가
            msg.attach(MIMEText(report_content, 'plain'))
            
            # SMTP 서버 연결
            server = smtplib.SMTP(email_config["smtp_server"], email_config["smtp_port"])
            server.starttls()
            server.login(email_config["username"], email_config["password"])
            
            # 이메일 발송
            server.send_message(msg)
            server.quit()
            
            logger.info(f"보고서 이메일 발송 완료: {', '.join(self.config['report_schedule']['recipients'])}")
            return True
            
        except Exception as e:
            logger.error(f"이메일 발송 중 오류 발생: {e}", exc_info=True)
            return False

class FileEventHandler(FileSystemEventHandler):
    """파일 시스템 이벤트 처리 핸들러"""
    
    def __init__(self, file_system):
        self.file_system = file_system
    
    def on_created(self, event):
        """파일 생성 이벤트 처리"""
        if not event.is_directory:
            logger.info(f"새 파일 감지됨: {event.src_path}")
            self.file_system.process_file(event.src_path)

# 메인 실행 코드
if __name__ == "__main__":
    # 파일 모니터링 시스템 생성
    monitor = FileMonitoringSystem("config.json")
    
    try:
        # 모니터링 시작
        monitor.start_monitoring()
        
        # 프로그램 실행 유지
        print("파일 모니터링 시스템이 실행 중입니다. 중지하려면 Ctrl+C를 누르세요.")
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("프로그램 종료 중...")
        monitor.stop_monitoring()
        print("종료되었습니다.")
```

## ✅ 14.6.2 프로젝트 활용법

이 프로젝트는 다양한 실제 시나리오에 활용할 수 있습니다:

1. **데이터 처리 자동화**: 데이터 파일이 특정 디렉토리에 도착하면 자동으로 처리하고 정리합니다.
2. **미디어 파일 관리**: 이미지, 오디오, 비디오 파일을 자동으로 분류하고 메타데이터를 추출합니다.
3. **로그 파일 분석**: 시스템에서 생성된 로그 파일을 자동으로 분석하고 보고서를 생성합니다.
4. **문서 관리 시스템**: 사무실 환경에서 문서 파일을 자동으로 분류하고 처리합니다.
5. **백업 시스템**: 중요한 파일을 정기적으로 백업하고 이전 백업을 관리합니다.

## ✅ 14.6.3 프로젝트 개선 방향

이 프로젝트는 다음과 같은 방향으로 확장 및 개선할 수 있습니다:

1. **웹 인터페이스 추가**: Flask나 Django를 사용하여 시스템 상태를 모니터링하고 설정을 변경할 수 있는 웹 인터페이스를 개발합니다.
2. **고급 파일 처리 기능**: 이미지 리사이징, OCR, 텍스트 마이닝 등 파일 유형별 고급 처리 기능을 추가합니다.
3. **분산 시스템으로 확장**: 여러 서버에서 파일을 모니터링하고 처리할 수 있도록 분산 시스템으로 확장합니다.
4. **머신러닝 통합**: 파일 분류, 이상 탐지, 콘텐츠 분석 등에 머신러닝 모델을 적용합니다.
5. **클라우드 통합**: AWS S3, Google Cloud Storage 등의 클라우드 스토리지와 통합하여 확장성을 높입니다.

이 프로젝트는 앞서 배운 파일 관리, 이메일 자동화, 웹 API 통합, 스케줄링 등의 개념을 종합적으로 활용한 실제 응용 사례입니다. 직접 구현하고 필요에 맞게 수정하여 실무에 적용해 보세요.

