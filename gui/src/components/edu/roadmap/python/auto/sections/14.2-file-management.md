# 14.2 파일 및 폴더 자동 관리

파일과 폴더 관리는 자동화의 가장 기본적이면서도 유용한 응용 분야입니다. 파이썬은 표준 라이브러리와 추가 패키지를 통해 강력한 파일 시스템 자동화 기능을 제공합니다.

## ✅ 14.2.1 파일 모니터링 및 자동 처리

특정 디렉토리를 감시하고 새로운 파일이 생성되면 자동으로 처리하는 기능은 자동화의 핵심입니다. watchdog 라이브러리를 사용하면 이러한 기능을 쉽게 구현할 수 있습니다.

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time
import os
import shutil
import logging

class FileHandler(FileSystemEventHandler):
    """파일 시스템 이벤트 처리 클래스"""
    
    def __init__(self, target_dir):
        self.target_dir = target_dir
    
    def on_created(self, event):
        """파일 생성 시 처리"""
        if not event.is_directory:
            file_name = os.path.basename(event.src_path)
            file_ext = os.path.splitext(file_name)[1]
            
            # 파일 종류별 분류
            if file_ext.lower() in ['.jpg', '.png', '.gif']:
                dest_dir = os.path.join(self.target_dir, 'images')
            elif file_ext.lower() in ['.doc', '.docx', '.pdf']:
                dest_dir = os.path.join(self.target_dir, 'documents')
            else:
                dest_dir = os.path.join(self.target_dir, 'others')
            
            os.makedirs(dest_dir, exist_ok=True)
            shutil.move(event.src_path, os.path.join(dest_dir, file_name))
            logging.info(f'파일 이동: {file_name} -> {dest_dir}')

def start_file_monitoring(path):
    """파일 모니터링 시작"""
    event_handler = FileHandler(path)
    observer = Observer()
    observer.schedule(event_handler, path, recursive=False)
    observer.start()
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()
```

이 코드는 지정된 디렉토리를 모니터링하고, 새 파일이 생성되면 확장자에 따라 적절한 하위 디렉토리로 자동 분류합니다.

## ✅ 14.2.2 대량 파일 처리 자동화

대량의 파일을 처리할 때는 효율성과 오류 처리가 중요합니다. 아래 예제는 파일 이름 일괄 변경, 중복 파일 찾기, 파일 내용 일괄 검색과 교체 기능을 구현한 것입니다.

```python
import os
import hashlib
import re
import filecmp
from concurrent.futures import ThreadPoolExecutor

class BulkFileProcessor:
    """대량 파일 처리를 위한 클래스"""
    
    def __init__(self, base_dir, max_workers=4):
        self.base_dir = base_dir
        self.max_workers = max_workers
    
    def rename_files_by_pattern(self, pattern, replacement, file_types=None):
        """정규식 패턴에 따라 파일 이름 일괄 변경"""
        renamed_count = 0
        
        for root, _, files in os.walk(self.base_dir):
            for filename in files:
                # 파일 유형 필터링
                if file_types and not any(filename.endswith(ft) for ft in file_types):
                    continue
                
                # 패턴 매칭 및 치환
                new_name = re.sub(pattern, replacement, filename)
                if new_name != filename:
                    old_path = os.path.join(root, filename)
                    new_path = os.path.join(root, new_name)
                    
                    try:
                        os.rename(old_path, new_path)
                        renamed_count += 1
                        print(f"파일 이름 변경: {filename} -> {new_name}")
                    except Exception as e:
                        print(f"이름 변경 실패 ({filename}): {e}")
        
        return renamed_count
    
    def find_duplicate_files(self):
        """해시 기반으로 중복 파일 찾기"""
        hash_dict = {}
        duplicates = []
        
        def calculate_hash(filepath):
            """파일의 MD5 해시 계산"""
            hash_md5 = hashlib.md5()
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        
        # 모든 파일 스캔 및 해시 계산
        for root, _, files in os.walk(self.base_dir):
            for filename in files:
                filepath = os.path.join(root, filename)
                try:
                    file_hash = calculate_hash(filepath)
                    if file_hash in hash_dict:
                        # 중복 발견
                        if filepath not in duplicates:
                            duplicates.append(filepath)
                        if hash_dict[file_hash] not in duplicates:
                            duplicates.append(hash_dict[file_hash])
                    else:
                        hash_dict[file_hash] = filepath
                except Exception as e:
                    print(f"파일 해시 계산 실패 ({filepath}): {e}")
        
        return duplicates
    
    def search_and_replace_in_files(self, search_pattern, replacement, file_types=None):
        """여러 파일에서 텍스트 검색 및 치환"""
        modified_files = []
        
        def process_file(filepath):
            """단일 파일 처리"""
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # 검색 패턴이 있는지 확인
                if re.search(search_pattern, content):
                    # 치환 수행
                    new_content = re.sub(search_pattern, replacement, content)
                    
                    # 파일에 저장
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    
                    return filepath
            except Exception as e:
                print(f"파일 처리 실패 ({filepath}): {e}")
            return None
        
        # 처리할 파일 목록 수집
        files_to_process = []
        for root, _, files in os.walk(self.base_dir):
            for filename in files:
                # 파일 유형 필터링
                if file_types and not any(filename.endswith(ft) for ft in file_types):
                    continue
                
                filepath = os.path.join(root, filename)
                files_to_process.append(filepath)
        
        # 병렬 처리
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = executor.map(process_file, files_to_process)
        
        # 결과 수집
        for result in results:
            if result:
                modified_files.append(result)
        
        return modified_files
```

## ✅ 14.2.3 자동 백업 및 데이터 보존 시스템

중요한 데이터를 정기적으로 백업하고 보존 정책에 따라 관리하는 것은 필수적인 작업입니다. 다음 예제는 자동 백업 시스템과 오래된 백업을 정리하는 기능을 구현한 것입니다.

```python
import os
import datetime
import shutil
import logging
import zipfile
import tarfile
import json

class AutoBackupSystem:
    """자동 백업 및 데이터 보존 시스템"""
    
    def __init__(self, config_file=None):
        """설정 파일을 통한 초기화"""
        # 기본 설정
        self.default_config = {
            "source_dirs": [],
            "backup_root": "./backups",
            "backup_format": "zip",  # 'zip' 또는 'tar'
            "retention_days": 30,
            "log_file": "backup.log"
        }
        
        # 설정 파일 로드
        self.config = self.default_config.copy()
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    user_config = json.load(f)
                self.config.update(user_config)
            except Exception as e:
                print(f"설정 파일 로드 실패: {e}")
        
        # 로깅 설정
        self.setup_logging()
    
    def setup_logging(self):
        """로깅 설정"""
        logging.basicConfig(
            filename=self.config["log_file"],
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('backup_system')
    
    def create_backup(self, source_dir=None):
        """지정된 디렉토리 백업"""
        source_dirs = [source_dir] if source_dir else self.config["source_dirs"]
        
        if not source_dirs:
            self.logger.error("백업할 소스 디렉토리가 지정되지 않았습니다.")
            return False
        
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = os.path.join(self.config["backup_root"], timestamp)
        
        # 백업 디렉토리 생성
        os.makedirs(backup_dir, exist_ok=True)
        
        for src_dir in source_dirs:
            if not os.path.exists(src_dir):
                self.logger.warning(f"소스 디렉토리가 존재하지 않습니다: {src_dir}")
                continue
            
            try:
                # 백업 파일 이름
                dir_name = os.path.basename(src_dir)
                backup_file = os.path.join(backup_dir, f"{dir_name}_{timestamp}")
                
                # 압축 형식에 따른 백업 생성
                if self.config["backup_format"] == "zip":
                    backup_file += ".zip"
                    self._create_zip_backup(src_dir, backup_file)
                else:
                    backup_file += ".tar.gz"
                    self._create_tar_backup(src_dir, backup_file)
                
                self.logger.info(f"백업 완료: {src_dir} -> {backup_file}")
            except Exception as e:
                self.logger.error(f"백업 실패 ({src_dir}): {e}")
                return False
        
        return True
    
    def _create_zip_backup(self, source_dir, backup_file):
        """ZIP 형식으로 백업"""
        with zipfile.ZipFile(backup_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, _, files in os.walk(source_dir):
                for file in files:
                    filepath = os.path.join(root, file)
                    arcname = os.path.relpath(filepath, os.path.dirname(source_dir))
                    zipf.write(filepath, arcname)
    
    def _create_tar_backup(self, source_dir, backup_file):
        """TAR.GZ 형식으로 백업"""
        with tarfile.open(backup_file, "w:gz") as tar:
            tar.add(source_dir, arcname=os.path.basename(source_dir))
    
    def cleanup_old_backups(self):
        """보존 기간을 초과한 백업 정리"""
        if not os.path.exists(self.config["backup_root"]):
            return
        
        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=self.config["retention_days"])
        
        for item in os.listdir(self.config["backup_root"]):
            item_path = os.path.join(self.config["backup_root"], item)
            
            # 디렉토리만 처리
            if os.path.isdir(item_path):
                try:
                    # 타임스탬프로부터 날짜 추출 (YYYYMMDD_HHMMSS 형식 가정)
                    date_str = item.split('_')[0]
                    if len(date_str) == 8:  # YYYYMMDD
                        year = int(date_str[:4])
                        month = int(date_str[4:6])
                        day = int(date_str[6:8])
                        
                        backup_date = datetime.datetime(year, month, day)
                        
                        # 오래된 백업 삭제
                        if backup_date < cutoff_date:
                            shutil.rmtree(item_path)
                            self.logger.info(f"오래된 백업 삭제: {item_path}")
                except (ValueError, IndexError) as e:
                    self.logger.warning(f"백업 날짜 분석 실패 ({item}): {e}")
    
    def schedule_backup(self, interval_days=1):
        """정기 백업 스케줄링 (간단한 예시)"""
        import time
        
        self.logger.info(f"백업 스케줄러 시작 (간격: {interval_days}일)")
        
        while True:
            try:
                # 백업 실행
                self.create_backup()
                
                # 오래된 백업 정리
                self.cleanup_old_backups()
                
                # 다음 백업까지 대기
                time.sleep(interval_days * 86400)  # 초 단위로 변환
            except KeyboardInterrupt:
                self.logger.info("백업 스케줄러 종료")
                break
            except Exception as e:
                self.logger.error(f"스케줄러 오류: {e}")
                # 오류 발생 시 잠시 대기 후 재시도
                time.sleep(3600)  # 1시간 대기
```

이 클래스는 다양한 디렉토리를 정기적으로 압축 백업하고, 지정된 보존 기간을 초과한 백업을 자동으로 정리합니다.

## ✅ 14.2.4 파일 동기화 도구

여러 위치 간에 파일을 동기화하는 것은 자동화의 또 다른 중요한 응용 분야입니다. 아래 예제는 두 디렉토리 간의 단방향 동기화를 구현한 것입니다.

```python
import os
import shutil
import hashlib
import logging
from datetime import datetime

class FileSync:
    """파일 동기화 도구"""
    
    def __init__(self, source_dir, target_dir, log_file=None):
        """초기화"""
        self.source_dir = source_dir
        self.target_dir = target_dir
        
        # 로깅 설정
        logging.basicConfig(
            filename=log_file or "sync.log",
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('file_sync')
    
    def calculate_file_hash(self, filepath):
        """파일 해시 계산"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def sync_directories(self, delete_extra=False):
        """디렉토리 동기화"""
        if not os.path.exists(self.source_dir):
            self.logger.error(f"소스 디렉토리가 존재하지 않습니다: {self.source_dir}")
            return False
        
        # 대상 디렉토리 생성 (없는 경우)
        os.makedirs(self.target_dir, exist_ok=True)
        
        copied_count = 0
        updated_count = 0
        deleted_count = 0
        
        # 소스 파일 처리
        for root, dirs, files in os.walk(self.source_dir):
            # 상대 경로 계산
            rel_path = os.path.relpath(root, self.source_dir)
            target_root = os.path.join(self.target_dir, rel_path) if rel_path != '.' else self.target_dir
            
            # 대상 디렉토리 생성
            os.makedirs(target_root, exist_ok=True)
            
            # 파일 복사/업데이트
            for file in files:
                source_file = os.path.join(root, file)
                target_file = os.path.join(target_root, file)
                
                # 파일이 없거나 크기/시간이 다른 경우 복사 대상으로 표시
                copy_needed = (
                    not os.path.exists(target_file) or
                    os.path.getsize(source_file) != os.path.getsize(target_file) or
                    os.path.getmtime(source_file) > os.path.getmtime(target_file)
                )
                
                if copy_needed:
                    try:
                        # 해시 비교 (파일 크기/시간은 다르지만 내용은 같을 수 있음)
                        if os.path.exists(target_file):
                            source_hash = self.calculate_file_hash(source_file)
                            target_hash = self.calculate_file_hash(target_file)
                            
                            if source_hash == target_hash:
                                continue  # 내용이 같으면 복사 불필요
                            
                            # 내용이 다르면 업데이트
                            shutil.copy2(source_file, target_file)
                            updated_count += 1
                            self.logger.info(f"파일 업데이트: {target_file}")
                        else:
                            # 새 파일 복사
                            shutil.copy2(source_file, target_file)
                            copied_count += 1
                            self.logger.info(f"파일 복사: {target_file}")
                    except Exception as e:
                        self.logger.error(f"파일 복사 실패 ({source_file}): {e}")
        
        # 소스에 없는 대상 파일 삭제 (옵션)
        if delete_extra:
            for root, dirs, files in os.walk(self.target_dir):
                # 상대 경로 계산
                rel_path = os.path.relpath(root, self.target_dir)
                source_root = os.path.join(self.source_dir, rel_path) if rel_path != '.' else self.source_dir
                
                # 소스에 없는 파일 삭제
                for file in files:
                    target_file = os.path.join(root, file)
                    source_file = os.path.join(source_root, file)
                    
                    if not os.path.exists(source_file):
                        try:
                            os.remove(target_file)
                            deleted_count += 1
                            self.logger.info(f"파일 삭제: {target_file}")
                        except Exception as e:
                            self.logger.error(f"파일 삭제 실패 ({target_file}): {e}")
        
        # 결과 요약
        summary = f"동기화 완료: {copied_count}개 복사, {updated_count}개 업데이트"
        if delete_extra:
            summary += f", {deleted_count}개 삭제"
        
        self.logger.info(summary)
        print(summary)
        
        return True
```

이 도구는 두 디렉토리 간의 파일을 효율적으로 동기화하며, 필요한 경우 소스에 없는 파일을 대상에서 삭제할 수도 있습니다.

파일 및 폴더 자동화는 다양한 프로젝트에서 활용될 수 있으며, 데이터 관리, 백업, 배포 등 여러 작업을 간소화하고 오류를 줄이는 데 도움이 됩니다. 다음 섹션에서는 이메일 및 메시지 자동화 방법에 대해 알아보겠습니다. 