# 14.4 웹 API 자동화

웹 자동화는 브라우저를 통한 웹 사이트 상호작용이나 API를 통한 웹 서비스 호출을 자동화하여 데이터 수집, 웹 테스트, 웹 사이트 모니터링 등 다양한 작업을 수행할 수 있게 해줍니다.

## ✅ 14.4.1 Selenium을 활용한 웹 자동화

Selenium은 웹 브라우저를 제어하여 브라우저 기반 애플리케이션을 자동화하는 강력한 도구입니다. 웹 페이지와의 상호작용, 양식 제출, 데이터 추출 등을 자동화할 수 있습니다.

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import logging
import time
import os

class WebAutomation:
    """웹 자동화 클래스"""
    
    def __init__(self, headless=False, chrome_options=None):
        """
        웹 자동화 초기화
        headless: 화면 없이 실행할지 여부
        chrome_options: 추가 Chrome 옵션
        """
        # 로깅 설정
        self.setup_logging()
        
        # 옵션 설정
        options = chrome_options or webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless')
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
        
        # 창 크기 설정 (모바일/태블릿 에뮬레이션 등에 유용)
        options.add_argument('--window-size=1920,1080')
        
        # 사용자 에이전트 설정 (필요한 경우)
        # options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36')
        
        # 브라우저 시작
        try:
            self.driver = webdriver.Chrome(options=options)
            self.wait = WebDriverWait(self.driver, 10)  # 기본 대기 시간 10초
            self.logger.info("Chrome 브라우저 시작됨")
        except Exception as e:
            self.logger.error(f"브라우저 시작 실패: {e}")
            raise
    
    def setup_logging(self):
        """로깅 설정"""
        logging.basicConfig(
            filename='web_automation.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('web_automation')
    
    def navigate_to(self, url):
        """웹 페이지 이동"""
        try:
            self.driver.get(url)
            self.logger.info(f"페이지 이동: {url}")
            return True
        except Exception as e:
            self.logger.error(f"페이지 이동 실패 ({url}): {e}")
            return False
    
    def login(self, url, username_field, password_field, username, password, submit_xpath=None):
        """웹사이트 로그인"""
        try:
            # 페이지 이동
            self.navigate_to(url)
            
            # 로그인 필드 대기 및 입력
            username_input = self.wait.until(
                EC.presence_of_element_located((By.NAME, username_field))
            )
            password_input = self.driver.find_element(By.NAME, password_field)
            
            username_input.send_keys(username)
            password_input.send_keys(password)
            
            # 제출 버튼이 지정된 경우
            if submit_xpath:
                submit_button = self.driver.find_element(By.XPATH, submit_xpath)
                submit_button.click()
            else:
                # 아니면 폼 제출
                password_input.submit()
            
            self.logger.info(f"로그인 시도: {username}")
            return True
        except Exception as e:
            self.logger.error(f"로그인 실패: {e}")
            return False
    
    def find_element(self, by, value, wait=True, timeout=10):
        """요소 찾기"""
        try:
            if wait:
                return WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((by, value))
                )
            else:
                return self.driver.find_element(by, value)
        except (TimeoutException, NoSuchElementException) as e:
            self.logger.warning(f"요소를 찾을 수 없음 ({by}={value}): {e}")
            return None
    
    def find_elements(self, by, value, wait=True, timeout=10):
        """요소 목록 찾기"""
        try:
            if wait:
                WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((by, value))
                )
            return self.driver.find_elements(by, value)
        except (TimeoutException, NoSuchElementException) as e:
            self.logger.warning(f"요소를 찾을 수 없음 ({by}={value}): {e}")
            return []
    
    def click_element(self, by, value, wait=True, timeout=10):
        """요소 클릭"""
        try:
            element = self.find_element(by, value, wait, timeout)
            if element:
                element.click()
                self.logger.info(f"요소 클릭: {by}={value}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"요소 클릭 실패 ({by}={value}): {e}")
            return False
    
    def fill_form(self, form_data):
        """폼 채우기
        form_data: 딕셔너리 {(by, value): 입력값, ...}
        """
        filled_fields = 0
        
        for (by, value), input_value in form_data.items():
            try:
                element = self.find_element(by, value)
                if element:
                    element.clear()  # 기존 내용 지우기
                    element.send_keys(input_value)
                    filled_fields += 1
                    self.logger.info(f"폼 필드 입력: {by}={value}")
            except Exception as e:
                self.logger.error(f"폼 필드 입력 실패 ({by}={value}): {e}")
        
        return filled_fields
    
    def scrape_data(self, elements_config):
        """데이터 스크래핑
        elements_config: 딕셔너리 {키: (by, value), ...}
        """
        result = {}
        
        for key, (by, value) in elements_config.items():
            elements = self.find_elements(by, value)
            
            if elements:
                # 여러 요소가 있는 경우 텍스트 리스트 수집
                if len(elements) > 1:
                    result[key] = [el.text for el in elements]
                # 단일 요소인 경우 텍스트만 수집
                else:
                    result[key] = elements[0].text
            else:
                result[key] = None
                self.logger.warning(f"스크래핑 데이터 없음: {key}")
        
        self.logger.info(f"데이터 스크래핑 완료: {len(result)} 항목")
        return result
    
    def take_screenshot(self, filename=None):
        """스크린샷 촬영"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            filename = f"screenshot_{timestamp}.png"
        
        try:
            # 스크린샷 디렉토리 생성
            os.makedirs("screenshots", exist_ok=True)
            filepath = os.path.join("screenshots", filename)
            
            # 스크린샷 저장
            self.driver.save_screenshot(filepath)
            self.logger.info(f"스크린샷 저장: {filepath}")
            return filepath
        except Exception as e:
            self.logger.error(f"스크린샷 촬영 실패: {e}")
            return None
    
    def execute_javascript(self, script, *args):
        """자바스크립트 실행"""
        try:
            return self.driver.execute_script(script, *args)
        except Exception as e:
            self.logger.error(f"자바스크립트 실행 실패: {e}")
            return None
    
    def wait_for_page_load(self, timeout=30):
        """페이지 로드 대기"""
        try:
            # document.readyState가 'complete'가 될 때까지 대기
            WebDriverWait(self.driver, timeout).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            self.logger.info("페이지 로드 완료")
            return True
        except TimeoutException:
            self.logger.warning(f"페이지 로드 타임아웃 ({timeout}초)")
            return False
    
    def close(self):
        """브라우저 종료"""
        try:
            self.driver.quit()
            self.logger.info("브라우저 종료됨")
        except Exception as e:
            self.logger.error(f"브라우저 종료 실패: {e}")
```

## ✅ 14.4.2 REST API를 활용한 자동화

REST API를 사용하면 웹 서비스와 직접 통신하여 데이터를 조회하고 수정할 수 있습니다. 파이썬의 requests 라이브러리를 활용해 API 호출을 자동화할 수 있습니다.

```python
import requests
import json
import time
import logging
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

class APIAutomation:
    """REST API 자동화 클래스"""
    
    def __init__(self, base_url=None, auth_token=None, username=None, password=None):
        """
        REST API 자동화 초기화
        base_url: API 기본 URL
        auth_token: 인증 토큰 (Bearer 인증 등에 사용)
        username, password: 기본 인증 (Basic Auth)에 사용
        """
        self.base_url = base_url
        self.auth_token = auth_token
        self.username = username
        self.password = password
        self.session = self._create_session()
        
        # 로깅 설정
        self.setup_logging()
    
    def setup_logging(self):
        """로깅 설정"""
        logging.basicConfig(
            filename='api_automation.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('api_automation')
    
    def _create_session(self):
        """요청 세션 생성 (재시도 로직 포함)"""
        session = requests.Session()
        
        # 재시도 설정
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "PUT", "POST", "DELETE", "OPTIONS", "TRACE"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # 기본 인증 설정
        if self.username and self.password:
            session.auth = (self.username, self.password)
        
        return session
    
    def _get_headers(self, additional_headers=None):
        """요청 헤더 생성"""
        headers = {'Content-Type': 'application/json'}
        
        # 인증 토큰 추가
        if self.auth_token:
            headers['Authorization'] = f'Bearer {self.auth_token}'
        
        # 추가 헤더 병합
        if additional_headers:
            headers.update(additional_headers)
        
        return headers
    
    def _build_url(self, endpoint):
        """전체 URL 생성"""
        if self.base_url:
            # 슬래시 처리
            if self.base_url.endswith('/') and endpoint.startswith('/'):
                endpoint = endpoint[1:]
            elif not self.base_url.endswith('/') and not endpoint.startswith('/'):
                endpoint = '/' + endpoint
            
            return f"{self.base_url}{endpoint}"
        return endpoint
    
    def get(self, endpoint, params=None, headers=None):
        """GET 요청"""
        url = self._build_url(endpoint)
        try:
            response = self.session.get(
                url,
                params=params,
                headers=self._get_headers(headers),
                timeout=(5, 30)  # 연결 5초, 읽기 30초 타임아웃
            )
            
            self._log_request("GET", url, response.status_code)
            response.raise_for_status()  # 4xx, 5xx 오류 시 예외 발생
            
            return response.json() if self._is_json(response) else response.text
        except requests.exceptions.RequestException as e:
            self.logger.error(f"GET 요청 실패 ({url}): {e}")
            return None
    
    def post(self, endpoint, data=None, json_data=None, params=None, headers=None):
        """POST 요청"""
        url = self._build_url(endpoint)
        try:
            response = self.session.post(
                url,
                data=data,
                json=json_data,
                params=params,
                headers=self._get_headers(headers),
                timeout=(5, 30)
            )
            
            self._log_request("POST", url, response.status_code)
            response.raise_for_status()
            
            return response.json() if self._is_json(response) else response.text
        except requests.exceptions.RequestException as e:
            self.logger.error(f"POST 요청 실패 ({url}): {e}")
            return None
    
    def put(self, endpoint, data=None, json_data=None, params=None, headers=None):
        """PUT 요청"""
        url = self._build_url(endpoint)
        try:
            response = self.session.put(
                url,
                data=data,
                json=json_data,
                params=params,
                headers=self._get_headers(headers),
                timeout=(5, 30)
            )
            
            self._log_request("PUT", url, response.status_code)
            response.raise_for_status()
            
            return response.json() if self._is_json(response) else response.text
        except requests.exceptions.RequestException as e:
            self.logger.error(f"PUT 요청 실패 ({url}): {e}")
            return None
    
    def delete(self, endpoint, params=None, headers=None):
        """DELETE 요청"""
        url = self._build_url(endpoint)
        try:
            response = self.session.delete(
                url,
                params=params,
                headers=self._get_headers(headers),
                timeout=(5, 30)
            )
            
            self._log_request("DELETE", url, response.status_code)
            response.raise_for_status()
            
            return response.json() if self._is_json(response) else response.text
        except requests.exceptions.RequestException as e:
            self.logger.error(f"DELETE 요청 실패 ({url}): {e}")
            return None
    
    def _is_json(self, response):
        """응답이 JSON인지 확인"""
        content_type = response.headers.get('Content-Type', '')
        return 'application/json' in content_type
    
    def _log_request(self, method, url, status_code):
        """요청 로깅"""
        self.logger.info(f"{method} {url} - 상태 코드: {status_code}")
    
    def auth_login(self, login_endpoint, username, password, token_field='token'):
        """인증 로그인 및 토큰 획득"""
        result = self.post(
            login_endpoint,
            json_data={'username': username, 'password': password}
        )
        
        if result and token_field in result:
            self.auth_token = result[token_field]
            self.logger.info("API 인증 토큰 획득 성공")
            return True
        else:
            self.logger.error("API 인증 토큰 획득 실패")
            return False
```

## ✅ 14.4.3 웹 크롤링 및 데이터 수집 자동화

웹 사이트에서 정보를 수집하는 크롤링 작업을 자동화하여 데이터 분석, 시장 조사, 경쟁사 모니터링 등 다양한 목적에 활용할 수 있습니다.

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import logging
import re
import os
from urllib.parse import urljoin

class WebCrawler:
    """웹 크롤링 및 데이터 수집 자동화"""
    
    def __init__(self, base_url=None, delay=1, randomize_delay=True, max_delay=3):
        """
        웹 크롤러 초기화
        base_url: 크롤링 기본 URL
        delay: 요청 간 대기 시간(초)
        randomize_delay: 대기 시간 무작위화 여부
        max_delay: 최대 대기 시간(randomize_delay가 True일 때 사용)
        """
        self.base_url = base_url
        self.delay = delay
        self.randomize_delay = randomize_delay
        self.max_delay = max_delay
        self.session = requests.Session()
        
        # 사용자 에이전트 설정 (차단 방지)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
        })
        
        # 로깅 설정
        self.setup_logging()
    
    def setup_logging(self):
        """로깅 설정"""
        logging.basicConfig(
            filename='web_crawler.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('web_crawler')
    
    def _wait(self):
        """요청 간 대기 (과도한 요청 방지)"""
        if self.randomize_delay:
            time.sleep(random.uniform(self.delay, self.max_delay))
        else:
            time.sleep(self.delay)
    
    def fetch_page(self, url, params=None):
        """웹 페이지 가져오기"""
        try:
            full_url = urljoin(self.base_url, url) if self.base_url else url
            self.logger.info(f"페이지 요청: {full_url}")
            
            response = self.session.get(full_url, params=params, timeout=10)
            response.raise_for_status()
            
            self._wait()  # 다음 요청 전 대기
            
            return response.text
        except requests.exceptions.RequestException as e:
            self.logger.error(f"페이지 요청 실패 ({url}): {e}")
            return None
    
    def parse_html(self, html_content):
        """HTML 파싱"""
        return BeautifulSoup(html_content, 'html.parser')
    
    def extract_data(self, soup, selectors):
        """선택자를 사용하여 데이터 추출
        selectors: 딕셔너리 {키: CSS 선택자, ...}
        """
        data = {}
        
        for key, selector in selectors.items():
            elements = soup.select(selector)
            
            if elements:
                # 여러 요소인 경우 텍스트 리스트로 추출
                if len(elements) > 1:
                    data[key] = [el.get_text(strip=True) for el in elements]
                # 단일 요소인 경우 텍스트만 추출
                else:
                    data[key] = elements[0].get_text(strip=True)
            else:
                data[key] = None
                self.logger.warning(f"선택자에 해당하는 요소 없음: {key} ({selector})")
        
        return data
    
    def extract_links(self, soup, link_selector='a', base_url=None):
        """링크 추출"""
        base = base_url or self.base_url
        links = []
        
        for a_tag in soup.select(link_selector):
            href = a_tag.get('href')
            if href:
                # 상대 URL을 절대 URL로 변환
                absolute_url = urljoin(base, href)
                links.append(absolute_url)
        
        return links
    
    def crawl_paginated_data(self, url_pattern, page_start=1, page_end=5, selectors=None):
        """페이지네이션이 있는 데이터 크롤링"""
        all_data = []
        
        for page in range(page_start, page_end + 1):
            # URL에 페이지 번호 삽입
            page_url = url_pattern.format(page=page)
            
            # 페이지 가져오기
            html_content = self.fetch_page(page_url)
            if not html_content:
                self.logger.warning(f"페이지 {page} 크롤링 실패, 건너뜀")
                continue
            
            # HTML 파싱
            soup = self.parse_html(html_content)
            
            # 데이터 추출
            if selectors:
                data = self.extract_data(soup, selectors)
                all_data.append(data)
                self.logger.info(f"페이지 {page} 데이터 추출 완료")
            
            # 과도한 요청 방지를 위한 추가 대기
            if page < page_end:
                self._wait()
        
        return all_data
    
    def crawl_recursive(self, start_url, depth=1, max_pages=10, link_selector='a', content_selectors=None):
        """재귀적 크롤링 (링크 따라가기)"""
        visited_urls = set()
        url_queue = [start_url]
        results = []
        page_count = 0
        
        while url_queue and page_count < max_pages:
            # 다음 URL 가져오기
            current_url = url_queue.pop(0)
            
            # 이미 방문한 URL은 건너뛰기
            if current_url in visited_urls:
                continue
            
            # 방문 기록
            visited_urls.add(current_url)
            page_count += 1
            
            # 페이지 가져오기
            html_content = self.fetch_page(current_url)
            if not html_content:
                continue
            
            # HTML 파싱
            soup = self.parse_html(html_content)
            
            # 콘텐츠 추출 (지정된 경우)
            if content_selectors:
                data = self.extract_data(soup, content_selectors)
                data['url'] = current_url
                results.append(data)
            
            # 최대 깊이에 도달하지 않았으면 링크 추출 및 큐에 추가
            if len(visited_urls) < depth:
                links = self.extract_links(soup, link_selector, current_url)
                # 방문하지 않은 링크만 큐에 추가
                for link in links:
                    if link not in visited_urls and link not in url_queue:
                        url_queue.append(link)
        
        self.logger.info(f"재귀적 크롤링 완료: {len(visited_urls)}개 URL 방문, {len(results)}개 결과")
        return results
    
    def save_to_csv(self, data, filename):
        """데이터를 CSV로 저장"""
        try:
            df = pd.DataFrame(data)
            
            # 디렉토리 생성
            os.makedirs(os.path.dirname(filename) or '.', exist_ok=True)
            
            df.to_csv(filename, index=False, encoding='utf-8')
            self.logger.info(f"데이터를 CSV로 저장: {filename} ({len(data)}개 항목)")
            
            return True
        except Exception as e:
            self.logger.error(f"CSV 저장 실패: {e}")
            return False
```

## ✅ 14.4.4 웹훅과 알림 자동화

웹훅(webhook)은 특정 이벤트가 발생했을 때 HTTP POST 요청을 보내는 방식으로, 다양한 시스템 간 연동 및 알림을 자동화하는 데 활용됩니다.

```python
import requests
import json
import hmac
import hashlib
import time
import logging
from datetime import datetime

class WebhookAutomation:
    """웹훅 및 알림 자동화"""
    
    def __init__(self):
        # 로깅 설정
        self.setup_logging()
        
        # 웹훅 설정 저장
        self.webhooks = {}
    
    def setup_logging(self):
        """로깅 설정"""
        logging.basicConfig(
            filename='webhook_automation.log',
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('webhook_automation')
    
    def register_webhook(self, name, url, headers=None, secret=None):
        """웹훅 등록"""
        self.webhooks[name] = {
            'url': url,
            'headers': headers or {},
            'secret': secret
        }
        self.logger.info(f"웹훅 등록: {name} -> {url}")
    
    def remove_webhook(self, name):
        """웹훅 제거"""
        if name in self.webhooks:
            del self.webhooks[name]
            self.logger.info(f"웹훅 제거: {name}")
            return True
        return False
    
    def _sign_payload(self, payload, secret):
        """HMAC을 사용하여 페이로드에 서명"""
        if not secret:
            return None
        
        # JSON 문자열로 변환
        payload_str = json.dumps(payload)
        
        # HMAC-SHA256 서명 생성
        signature = hmac.new(
            secret.encode('utf-8'),
            payload_str.encode('utf-8'),
            hashlib.sha256
        ).hexdigest()
        
        return signature
    
    def trigger_webhook(self, name, payload, include_timestamp=True):
        """웹훅 트리거"""
        if name not in self.webhooks:
            self.logger.error(f"등록되지 않은 웹훅: {name}")
            return False
        
        webhook = self.webhooks[name]
        
        # 타임스탬프 추가
        if include_timestamp:
            payload['timestamp'] = datetime.now().isoformat()
        
        # 서명 생성 (시크릿이 있는 경우)
        signature = self._sign_payload(payload, webhook.get('secret'))
        
        # 헤더 설정
        headers = {
            'Content-Type': 'application/json',
            **webhook['headers']
        }
        
        # 서명 헤더 추가
        if signature:
            headers['X-Webhook-Signature'] = signature
        
        try:
            # POST 요청 전송
            response = requests.post(
                webhook['url'],
                data=json.dumps(payload),
                headers=headers,
                timeout=10
            )
            
            # 응답 확인
            if response.status_code in (200, 201, 202, 204):
                self.logger.info(f"웹훅 트리거 성공: {name} - 상태 코드: {response.status_code}")
                return True
            else:
                self.logger.error(f"웹훅 트리거 실패: {name} - 상태 코드: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            self.logger.error(f"웹훅 요청 오류: {name} - {e}")
            return False
    
    def scheduled_webhook(self, name, payload, interval_seconds, max_triggers=None):
        """정기적인 웹훅 트리거"""
        self.logger.info(f"스케줄 웹훅 시작: {name} (간격: {interval_seconds}초)")
        
        trigger_count = 0
        
        try:
            while max_triggers is None or trigger_count < max_triggers:
                # 웹훅 트리거
                success = self.trigger_webhook(name, payload.copy())
                
                if success:
                    trigger_count += 1
                    self.logger.info(f"스케줄 웹훅 트리거 #{trigger_count}: {name}")
                
                # 다음 트리거까지 대기
                time.sleep(interval_seconds)
        except KeyboardInterrupt:
            self.logger.info(f"스케줄 웹훅 중단: {name}")
        
        self.logger.info(f"스케줄 웹훅 종료: {name} - 총 {trigger_count}회 트리거")
        return trigger_count
```

웹 API 자동화를 통해 다양한 온라인 서비스와 연동하고, 데이터 수집, 웹 테스트, 알림 등 다양한 작업을 자동화할 수 있습니다. 다음 섹션에서는 작업 스케줄링과 배치 스크립트를 통한 정기적인 자동화 작업 실행 방법에 대해 알아보겠습니다. 