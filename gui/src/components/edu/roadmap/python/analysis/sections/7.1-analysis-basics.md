# 7.1 데이터 분석 개념 및 기초

## ✅ 7.1.1 데이터 분석의 단계

데이터 분석은 일반적으로 다음과 같은 단계로 진행됩니다:

1. **데이터 수집**
   - 웹 크롤링: 웹 페이지에서 자동으로 데이터 추출
   - API 활용: 다양한 서비스의 API를 통해 데이터 획득
   - 데이터베이스 조회: SQL 등을 사용하여 기존 데이터베이스에서 데이터 추출
   - 파일 불러오기: CSV, Excel, JSON 등 다양한 형식의 파일에서 데이터 로드

2. **데이터 전처리**
   - 결측값 처리: 누락된 데이터 식별 및 대체/제거
   - 이상치 제거: 통계적으로 비정상적인 값 식별 및 처리
   - 데이터 정규화: 서로 다른 스케일의 데이터를 비교 가능하게 변환
   - 데이터 변환: 범주형 데이터의 인코딩, 날짜/시간 형식 변환 등

3. **데이터 분석**
   - 기술 통계: 평균, 중앙값, 표준편차 등 기본 통계량 계산
   - 상관 분석: 변수 간 관계 파악
   - 가설 검정: 데이터가 특정 가설을 지지하는지 통계적으로 검증
   - 그룹화 분석: 범주별 데이터 특성 비교

4. **결과 시각화**
   - 그래프 작성: 막대 그래프, 선 그래프, 산점도 등 다양한 그래프 제작
   - 대시보드 구성: 여러 시각화를 한 화면에 배치하여 종합적 정보 제공
   - 보고서 작성: 분석 결과를 정리하여 다른 사람들이 이해할 수 있는 형태로 제작

```python
import numpy as np
import pandas as pd

# 기본 데이터 분석 클래스
class DataAnalyzer:
    def __init__(self, data):
        self.data = data
        self.df = pd.DataFrame(data)
    
    def basic_statistics(self):
        """기본 통계 분석"""
        stats = {
            'mean': np.mean(self.data),
            'median': np.median(self.data),
            'std': np.std(self.data),
            'min': np.min(self.data),
            'max': np.max(self.data)
        }
        return stats
    
    def check_missing_values(self):
        """결측값 확인"""
        return self.df.isnull().sum()
    
    def detect_outliers(self, column, threshold=3):
        """이상치 탐지 (Z-score 방법)"""
        z_scores = np.abs((self.df[column] - self.df[column].mean()) / self.df[column].std())
        return self.df[z_scores > threshold]
```

## ✅ 7.1.2 기본 통계 개념

데이터 분석에 자주 사용되는 주요 통계 개념들입니다:

1. **중심 경향성 측정**
   - **평균(Mean)**: 모든 값의 합을 개수로 나눈 값
   - **중앙값(Median)**: 모든 값을 정렬했을 때 중앙에 위치한 값
   - **최빈값(Mode)**: 가장 자주 등장하는 값

2. **퍼짐 정도 측정**
   - **범위(Range)**: 최댓값과 최솟값의 차이
   - **분산(Variance)**: 각 데이터가 평균에서 얼마나 떨어져 있는지의 제곱 평균
   - **표준편차(Standard Deviation)**: 분산의 제곱근, 데이터의 퍼짐 정도를 원래 단위로 표현
   - **사분위수 범위(IQR)**: 데이터를 정렬했을 때 25%와 75% 지점 값의 차이

3. **분포 특성**
   - **왜도(Skewness)**: 분포의 비대칭 정도
   - **첨도(Kurtosis)**: 분포의 뾰족한 정도

```python
def distribution_analysis(data):
    """데이터 분포 분석"""
    import scipy.stats as stats
    
    analysis = {
        # 중심 경향성
        'mean': np.mean(data),
        'median': np.median(data),
        'mode': stats.mode(data, keepdims=True)[0][0],
        
        # 퍼짐 정도
        'range': np.max(data) - np.min(data),
        'variance': np.var(data),
        'std_dev': np.std(data),
        'iqr': np.percentile(data, 75) - np.percentile(data, 25),
        
        # 분포 특성
        'skewness': stats.skew(data),
        'kurtosis': stats.kurtosis(data)
    }
    
    return analysis
```

## ✅ 7.1.3 데이터 전처리 기법

효과적인 데이터 분석을 위한 주요 전처리 기법입니다:

1. **결측값 처리**
   - **제거**: 결측값이 있는 행/열 삭제
   - **대체**: 평균, 중앙값, 최빈값 등으로 결측값 대체
   - **예측**: 머신러닝 모델을 사용해 결측값 예측

2. **이상치 처리**
   - **Z-score 방법**: 평균에서 표준편차의 일정 배수 이상 벗어난 값 식별
   - **IQR 방법**: Q1-1.5×IQR 미만 또는 Q3+1.5×IQR 초과 값 식별
   - **제거/변환**: 이상치 제거 또는 경계값으로 대체

3. **데이터 정규화/표준화**
   - **Min-Max 정규화**: 모든 값을 0~1 범위로 변환
   - **Z-score 표준화**: 평균이 0, 표준편차가 1이 되도록 변환
   - **로그 변환**: 왜곡된 분포를 정규 분포에 가깝게 변환

```python
def preprocess_data(df):
    """데이터 전처리 함수"""
    # 결측값 처리
    df_clean = df.copy()
    
    # 수치형 열은 중앙값으로 대체
    numeric_cols = df_clean.select_dtypes(include=['number']).columns
    for col in numeric_cols:
        df_clean[col].fillna(df_clean[col].median(), inplace=True)
    
    # 범주형 열은 최빈값으로 대체
    categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns
    for col in categorical_cols:
        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)
    
    # 이상치 처리 (IQR 방법)
    for col in numeric_cols:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # 이상치를 경계값으로 대체
        df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])
        df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])
    
    # 표준화
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    df_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])
    
    return df_clean
```

## ✅ 7.1.4 데이터 탐색 기법

데이터의 특성을 파악하기 위한 탐색 기법입니다:

1. **단변량 분석**
   - **수치형 데이터**: 히스토그램, 상자 그림, 밀도 플롯
   - **범주형 데이터**: 막대 그래프, 파이 차트

2. **이변량 분석**
   - **수치형 vs 수치형**: 산점도, 선 그래프, 상관관계 분석
   - **범주형 vs 범주형**: 모자이크 플롯, 교차표(크로스탭)
   - **수치형 vs 범주형**: 박스플롯, 바이올린 플롯

3. **다변량 분석**
   - **상관관계 행렬/히트맵**: 여러 변수 간 상관관계 시각화
   - **산점도 행렬**: 다수의 변수 쌍을 동시에 시각화
   - **평행 좌표 그래프**: 여러 차원의 데이터를 2D로 표현

```python
def explore_data(df):
    """데이터 탐색 함수"""
    # 기본 정보
    print("== 데이터 기본 정보 ==")
    print(f"행 수: {df.shape[0]}, 열 수: {df.shape[1]}")
    print("\n== 데이터 타입 ==")
    print(df.dtypes)
    
    # 요약 통계량
    print("\n== 요약 통계량 ==")
    print(df.describe())
    
    # 결측값 확인
    print("\n== 결측값 개수 ==")
    print(df.isnull().sum())
    
    # 상관관계 분석 (수치형 데이터만)
    numeric_df = df.select_dtypes(include=['number'])
    if not numeric_df.empty:
        print("\n== 상관관계 (Pearson) ==")
        corr = numeric_df.corr()
        print(corr)
    
    return {
        'summary': df.describe(),
        'missing': df.isnull().sum(),
        'correlation': corr if 'corr' in locals() else None
    }
```

## ✅ 7.1.5 데이터 분석 프로젝트 구조화

효과적인 데이터 분석 프로젝트 구조화 방법입니다:

1. **문제 정의**
   - 명확한 분석 목표 설정
   - 핵심 질문 목록 작성
   - 필요한 데이터와 자원 식별

2. **데이터 준비 및 탐색**
   - 데이터 수집 및 통합
   - 기초 탐색 및 시각화
   - 데이터 품질 평가 및 개선

3. **분석 및 모델링**
   - 적절한 분석 기법 선택
   - 가설 검정 및 패턴 식별
   - 필요시 예측 모델 구축

4. **결과 해석 및 커뮤니케이션**
   - 주요 발견사항 정리
   - 시각적 표현 및 대시보드 제작
   - 비즈니스 관점에서의 함의 도출

```python
def data_analysis_project(data_path, target_variable=None):
    """데이터 분석 프로젝트 구조 예시"""
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # 1. 데이터 로드
    print("1. 데이터 로드 중...")
    try:
        if data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        elif data_path.endswith('.xlsx') or data_path.endswith('.xls'):
            df = pd.read_excel(data_path)
        elif data_path.endswith('.json'):
            df = pd.read_json(data_path)
        else:
            raise ValueError("지원하지 않는 파일 형식입니다.")
        
        print(f"데이터 로드 완료: {df.shape[0]}행 x {df.shape[1]}열")
    except Exception as e:
        print(f"데이터 로드 실패: {str(e)}")
        return None
    
    # 2. 데이터 탐색
    print("\n2. 데이터 탐색 중...")
    explore_results = explore_data(df)
    
    # 3. 데이터 전처리
    print("\n3. 데이터 전처리 중...")
    df_clean = preprocess_data(df)
    print("전처리 완료")
    
    # 4. 데이터 시각화
    print("\n4. 주요 시각화 생성 중...")
    plt.figure(figsize=(15, 10))
    
    # 4.1. 수치형 데이터 분포
    numeric_cols = df_clean.select_dtypes(include=['number']).columns[:4]  # 처음 4개만
    for i, col in enumerate(numeric_cols):
        plt.subplot(2, 2, i+1)
        sns.histplot(df_clean[col], kde=True)
        plt.title(f'{col} 분포')
    
    plt.tight_layout()
    
    # 5. 목표 변수 분석 (지정된 경우)
    if target_variable and target_variable in df_clean.columns:
        print(f"\n5. 목표 변수 '{target_variable}' 분석 중...")
        
        # 목표 변수가 범주형인 경우
        if df_clean[target_variable].dtype == 'object' or df_clean[target_variable].nunique() < 10:
            print(f"목표 변수 분포:\n{df_clean[target_variable].value_counts()}")
            
            # 각 변수와 목표 변수의 관계 분석
            for col in df_clean.select_dtypes(include=['number']).columns[:3]:
                print(f"\n{col}와(과) {target_variable}의 관계:")
                plt.figure(figsize=(10, 6))
                sns.boxplot(x=target_variable, y=col, data=df_clean)
                plt.title(f'{col} by {target_variable}')
                plt.tight_layout()
        
        # 목표 변수가 수치형인 경우
        else:
            print(f"목표 변수 통계:\n{df_clean[target_variable].describe()}")
            
            # 상관관계 분석
            numeric_df = df_clean.select_dtypes(include=['number'])
            plt.figure(figsize=(12, 10))
            sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
            plt.title('변수 간 상관관계')
            plt.tight_layout()
    
    # 6. 결과 요약
    print("\n6. 분석 요약")
    print("- 데이터셋 크기:", df.shape)
    print("- 결측값 비율:", df.isnull().sum().sum() / (df.shape[0] * df.shape[1]))
    
    if target_variable and target_variable in df_clean.columns:
        if df_clean[target_variable].dtype != 'object':
            # 주요 상관관계 출력 (수치형 목표 변수인 경우)
            correlations = numeric_df.corr()[target_variable].sort_values(ascending=False)
            print(f"\n- {target_variable}와(과) 상관관계가 높은 상위 3개 변수:")
            print(correlations[1:4])  # 자기 자신 제외하고 상위 3개
    
    return {
        'original_data': df,
        'cleaned_data': df_clean,
        'exploration_results': explore_results
    } 