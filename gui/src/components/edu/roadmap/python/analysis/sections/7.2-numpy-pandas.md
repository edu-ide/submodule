# 7.2 NumPy와 Pandas를 사용한 데이터 처리

## ✅ 7.2.1 NumPy 고급 기능

NumPy는 파이썬의 고성능 수치 계산을 위한 핵심 라이브러리로, 효율적인 다차원 배열 처리 기능을 제공합니다.

### 다차원 배열 연산

```python
import numpy as np

# 다차원 배열 생성
arr_1d = np.array([1, 2, 3, 4, 5])
arr_2d = np.array([[1, 2, 3], [4, 5, 6]])
arr_3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

print("1차원 배열:", arr_1d.shape)  # (5,)
print("2차원 배열:", arr_2d.shape)  # (2, 3)
print("3차원 배열:", arr_3d.shape)  # (2, 2, 2)

# 배열 인덱싱과 슬라이싱
print("2D 배열 첫 행:", arr_2d[0])       # [1 2 3]
print("2D 배열 특정 요소:", arr_2d[1, 2])  # 6
print("3D 배열 슬라이스:", arr_3d[:, 0, :])  # [[1 2], [5 6]]

# 배열 재구성
reshaped = arr_1d.reshape(5, 1)
print("재구성된 배열:", reshaped.shape)  # (5, 1)

# 배열 연결
horizontal = np.hstack((arr_2d, arr_2d))
vertical = np.vstack((arr_2d, arr_2d))
print("수평 연결:", horizontal.shape)  # (2, 6)
print("수직 연결:", vertical.shape)    # (4, 3)
```

### 브로드캐스팅

NumPy의 브로드캐스팅은 다른 형태의 배열 간 연산을 자동으로 처리해주는 강력한 기능입니다.

```python
# 스칼라와 배열 연산
arr = np.array([1, 2, 3, 4])
result = arr * 2
print("스칼라 곱셈:", result)  # [2 4 6 8]

# 다른 크기의 배열 간 연산
arr_1 = np.array([[1, 2, 3], [4, 5, 6]])  # (2, 3)
arr_2 = np.array([10, 20, 30])            # (3,)
result = arr_1 + arr_2
print("브로드캐스팅 결과:")
print(result)  # [[11 22 33], [14 25 36]]

# 행렬과 열벡터 연산
matrix = np.array([[1, 2], [3, 4], [5, 6]])  # (3, 2)
col_vector = np.array([[10], [20]])          # (2, 1)
result = matrix.dot(col_vector)
print("행렬-벡터 곱:")
print(result)  # [[50], [110], [170]]
```

### 행렬 연산

NumPy를 사용한 효율적인 행렬 연산은 데이터 분석과 과학 계산에 필수적입니다.

```python
# 행렬 생성
matrix_a = np.array([[1, 2], [3, 4]])
matrix_b = np.array([[5, 6], [7, 8]])

# 행렬 덧셈과 뺄셈
print("행렬 덧셈:")
print(matrix_a + matrix_b)  # [[6 8], [10 12]]

print("행렬 뺄셈:")
print(matrix_a - matrix_b)  # [[-4 -4], [-4 -4]]

# 행렬 곱
print("행렬 곱:")
print(np.dot(matrix_a, matrix_b))  # [[19 22], [43 50]]
# 또는 더 간단히
print(matrix_a @ matrix_b)  # [[19 22], [43 50]]

# 요소별 곱 (Hadamard 곱)
print("요소별 곱:")
print(matrix_a * matrix_b)  # [[5 12], [21 32]]

# 행렬 전치
print("전치 행렬:")
print(matrix_a.T)  # [[1 3], [2 4]]

# 역행렬
print("역행렬:")
inv_a = np.linalg.inv(matrix_a)
print(inv_a)
print("원래 행렬과 역행렬의 곱:")
print(np.dot(matrix_a, inv_a))  # 단위 행렬에 가까움

# 행렬식
det_a = np.linalg.det(matrix_a)
print("행렬식:", det_a)  # -2.0

# 고유값과 고유벡터
eigenvalues, eigenvectors = np.linalg.eig(matrix_a)
print("고유값:", eigenvalues)
print("고유벡터:")
print(eigenvectors)
```

## ✅ 7.2.2 Pandas 기본 기능

Pandas는 데이터 조작과 분석을 위한 강력한 라이브러리로, 데이터프레임과 시리즈라는 핵심 자료구조를 제공합니다.

### 시리즈(Series)와 데이터프레임(DataFrame) 생성

```python
import pandas as pd
import numpy as np

# 시리즈 생성
series = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])
print("Pandas Series:")
print(series)

# 딕셔너리에서 데이터프레임 생성
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
    'Age': [25, 30, 35, 40, 45],
    'City': ['New York', 'Paris', 'London', 'Tokyo', 'Sydney']
}
df = pd.DataFrame(data)
print("\nPandas DataFrame:")
print(df)

# NumPy 배열에서 데이터프레임 생성
array_data = np.random.randn(5, 3)
df_array = pd.DataFrame(array_data, columns=['A', 'B', 'C'])
print("\nDataFrame from NumPy array:")
print(df_array)

# CSV 파일에서 불러오기
# df_csv = pd.read_csv('data.csv')
# Excel 파일에서 불러오기
# df_excel = pd.read_excel('data.xlsx')
```

### 데이터 탐색

```python
# 데이터프레임 기본 정보
print("데이터프레임 크기:", df.shape)
print("\n처음 몇 행:")
print(df.head(3))
print("\n마지막 몇 행:")
print(df.tail(2))

# 기본 통계 정보
print("\n기술 통계량:")
print(df.describe())

# 열 데이터 타입
print("\n데이터 타입:")
print(df.dtypes)

# 인덱스와 컬럼
print("\n인덱스:", df.index)
print("컬럼:", df.columns)

# 특정 열 선택
print("\n특정 열 선택:")
print(df['Name'])
print(df.Name)  # 속성으로도 접근 가능

# 여러 열 선택
print("\n여러 열 선택:")
print(df[['Name', 'Age']])

# 조건부 선택
print("\n30세 이상인 사람:")
print(df[df['Age'] >= 30])
```

### 데이터 정리 및 변환

```python
# 결측값 처리
data_with_na = {
    'A': [1, 2, np.nan, 4, 5],
    'B': [np.nan, 2, 3, 4, 5],
    'C': [1, 2, 3, np.nan, 5]
}
df_na = pd.DataFrame(data_with_na)
print("결측값이 있는 데이터프레임:")
print(df_na)

# 결측값 확인
print("\n결측값 확인:")
print(df_na.isnull().sum())

# 결측값 대체
print("\n결측값을 0으로 대체:")
print(df_na.fillna(0))

# 결측값이 있는 행 제거
print("\n결측값이 있는 행 제거:")
print(df_na.dropna())

# 결측값이 있는 열 제거
print("\n결측값이 있는 열 제거:")
print(df_na.dropna(axis=1))

# 데이터 타입 변환
df['Age'] = df['Age'].astype(float)
print("\n나이를 실수형으로 변환:")
print(df.dtypes)

# 범주형 데이터 변환
df['City'] = df['City'].astype('category')
print("\n도시를 범주형으로 변환:")
print(df.dtypes)
```

## ✅ 7.2.3 Pandas 고급 기능

Pandas의 강력한 데이터 조작 기능을 활용하면 복잡한 데이터 처리 작업을 효율적으로 수행할 수 있습니다.

### 그룹화와 집계

```python
# 더 많은 데이터 생성
data = {
    'Category': ['A', 'B', 'A', 'C', 'B', 'A', 'C'],
    'Value': [10, 20, 15, 25, 30, 5, 40],
    'Status': ['Active', 'Inactive', 'Active', 'Active', 'Active', 'Inactive', 'Inactive']
}
df = pd.DataFrame(data)
print("샘플 데이터:")
print(df)

# 범주별 집계
print("\n범주별 평균 값:")
print(df.groupby('Category')['Value'].mean())

# 다중 집계 함수
print("\n범주별 다양한 통계:")
print(df.groupby('Category')['Value'].agg(['min', 'max', 'mean', 'count']))

# 다중 열 그룹화
print("\n범주와 상태별 집계:")
print(df.groupby(['Category', 'Status'])['Value'].mean())

# 그룹 객체의 반복
print("\n각 그룹의 정보:")
for name, group in df.groupby('Category'):
    print(f"Group: {name}")
    print(group)
    print()
```

### 피벗 테이블과 크로스탭

```python
# 피벗 테이블
pivot = df.pivot_table(
    values='Value',
    index='Category',
    columns='Status',
    aggfunc='mean',
    fill_value=0
)
print("피벗 테이블:")
print(pivot)

# 크로스탭 (교차표)
crosstab = pd.crosstab(
    df['Category'],
    df['Status'],
    values=df['Value'],
    aggfunc='sum',
    margins=True,
    margins_name='Total'
)
print("\n크로스탭:")
print(crosstab)
```

### 병합과 연결

```python
# 데이터프레임 병합 예제
df1 = pd.DataFrame({
    'ID': [1, 2, 3, 4],
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40]
})

df2 = pd.DataFrame({
    'ID': [1, 2, 3, 5],
    'City': ['New York', 'Paris', 'London', 'Tokyo'],
    'Salary': [50000, 60000, 70000, 80000]
})

print("첫 번째 데이터프레임:")
print(df1)
print("\n두 번째 데이터프레임:")
print(df2)

# 내부 조인 (양쪽 모두에 있는 키만)
inner_join = pd.merge(df1, df2, on='ID', how='inner')
print("\n내부 조인:")
print(inner_join)

# 왼쪽 조인 (df1의 모든 행 유지)
left_join = pd.merge(df1, df2, on='ID', how='left')
print("\n왼쪽 조인:")
print(left_join)

# 오른쪽 조인 (df2의 모든 행 유지)
right_join = pd.merge(df1, df2, on='ID', how='right')
print("\n오른쪽 조인:")
print(right_join)

# 외부 조인 (모든 행 유지)
outer_join = pd.merge(df1, df2, on='ID', how='outer')
print("\n외부 조인:")
print(outer_join)

# 수직 연결
df3 = pd.DataFrame({
    'ID': [6, 7],
    'Name': ['Eve', 'Frank'],
    'Age': [45, 50]
})

concat_df = pd.concat([df1, df3])
print("\n수직 연결:")
print(concat_df)
```

### 시계열 데이터 처리

```python
# 시계열 데이터 생성
dates = pd.date_range('2023-01-01', periods=10, freq='D')
ts = pd.Series(np.random.randn(10), index=dates)
print("시계열 데이터:")
print(ts)

# 날짜 범위 선택
print("\n특정 기간 선택:")
print(ts['2023-01-03':'2023-01-07'])

# 시간 단위 변경 (리샘플링)
print("\n주간 평균:")
print(ts.resample('W').mean())

# 시프트 (이동)
print("\n하루 뒤로 이동:")
print(ts.shift(1))

# 차분
print("\n차분(차이값):")
print(ts.diff())

# 롤링 윈도우
print("\n3일 이동 평균:")
print(ts.rolling(window=3).mean())
```

## ✅ 7.2.4 효율적인 데이터 처리 기법

대용량 데이터와 복잡한 데이터 처리 작업에 유용한 기법들입니다.

### 벡터화 연산

```python
import time
import numpy as np
import pandas as pd

# 데이터 생성
size = 1000000
array = np.random.randn(size)
df = pd.DataFrame({'A': array, 'B': array * 2})

# 반복문 vs 벡터화 연산 비교
def loop_method():
    result = np.zeros(size)
    for i in range(size):
        result[i] = df['A'][i] * 2 + df['B'][i]
    return result

def vectorized_method():
    return df['A'] * 2 + df['B']

# 성능 비교
start = time.time()
loop_result = loop_method()
loop_time = time.time() - start

start = time.time()
vectorized_result = vectorized_method()
vectorized_time = time.time() - start

print(f"반복문 방식: {loop_time:.4f}초")
print(f"벡터화 방식: {vectorized_time:.4f}초")
print(f"속도 향상: {loop_time / vectorized_time:.1f}배")
```

### apply, map, applymap 활용

```python
# 중간 크기의 데이터프레임
df = pd.DataFrame({
    'A': np.random.randint(0, 100, 10),
    'B': np.random.randint(0, 100, 10),
    'C': np.random.choice(['X', 'Y', 'Z'], 10)
})
print("원본 데이터:")
print(df)

# Series에 map 적용
print("\nC 열의 매핑 변환:")
mapping = {'X': 'Category 1', 'Y': 'Category 2', 'Z': 'Category 3'}
print(df['C'].map(mapping))

# Series에 apply 적용
print("\nA 열에 함수 적용:")
def process_value(x):
    if x < 30:
        return 'Low'
    elif x < 70:
        return 'Medium'
    else:
        return 'High'

print(df['A'].apply(process_value))

# DataFrame에 apply 적용 (행 또는 열 단위)
print("\n행 방향으로 최대값:")
print(df[['A', 'B']].apply(np.max, axis=1))

print("\n열 방향으로 최소값:")
print(df[['A', 'B']].apply(np.min, axis=0))

# DataFrame에 applymap 적용 (모든 요소)
print("\n모든 숫자에 제곱 함수 적용:")
print(df[['A', 'B']].applymap(lambda x: x**2))
```

### 메모리 사용 최적화

```python
# 데이터 타입 최적화
def optimize_dtypes(df):
    """데이터프레임의 메모리 사용량을 최적화"""
    df_optimized = df.copy()
    
    # 정수형 열 최적화
    int_columns = df_optimized.select_dtypes(include=['int']).columns
    for col in int_columns:
        col_min = df_optimized[col].min()
        col_max = df_optimized[col].max()
        
        # 적절한 정수 타입 선택
        if col_min >= 0:
            if col_max < 2**8:
                df_optimized[col] = df_optimized[col].astype(np.uint8)
            elif col_max < 2**16:
                df_optimized[col] = df_optimized[col].astype(np.uint16)
            elif col_max < 2**32:
                df_optimized[col] = df_optimized[col].astype(np.uint32)
        else:
            if col_min > -2**7 and col_max < 2**7:
                df_optimized[col] = df_optimized[col].astype(np.int8)
            elif col_min > -2**15 and col_max < 2**15:
                df_optimized[col] = df_optimized[col].astype(np.int16)
            elif col_min > -2**31 and col_max < 2**31:
                df_optimized[col] = df_optimized[col].astype(np.int32)
    
    # 부동소수점 열 최적화
    float_columns = df_optimized.select_dtypes(include=['float']).columns
    for col in float_columns:
        df_optimized[col] = df_optimized[col].astype(np.float32)
    
    # 범주형 변환 (고유값이 적은 경우)
    obj_columns = df_optimized.select_dtypes(include=['object']).columns
    for col in obj_columns:
        if df_optimized[col].nunique() / len(df_optimized) < 0.5:  # 고유값 비율이 50% 미만이면 범주형으로
            df_optimized[col] = df_optimized[col].astype('category')
    
    return df_optimized

# 예시 데이터
df_large = pd.DataFrame({
    'int_col': np.random.randint(0, 100, 100000),
    'float_col': np.random.randn(100000),
    'str_col': np.random.choice(['A', 'B', 'C', 'D', 'E'], 100000)
})

# 최적화 전후 메모리 사용량 비교
def get_memory_usage(df):
    return df.memory_usage(deep=True).sum() / (1024 * 1024)  # MB 단위

print(f"최적화 전 메모리 사용량: {get_memory_usage(df_large):.2f} MB")
df_optimized = optimize_dtypes(df_large)
print(f"최적화 후 메모리 사용량: {get_memory_usage(df_optimized):.2f} MB")
print(f"메모리 절약: {get_memory_usage(df_large) / get_memory_usage(df_optimized):.1f}배")
```

### 청크 단위 처리

대용량 데이터를 메모리 효율적으로 처리하기 위한 청크 단위 처리 방법입니다.

```python
def process_large_csv(filename, chunk_size=10000):
    """대용량 CSV 파일을 청크 단위로 처리"""
    # 결과를 저장할 리스트
    results = []
    
    # 청크 단위로 데이터 읽기
    for chunk in pd.read_csv(filename, chunksize=chunk_size):
        # 각 청크에 대한 처리 로직
        processed_chunk = chunk.groupby('category')['value'].mean()
        results.append(processed_chunk)
    
    # 모든 청크의 결과 합치기
    final_result = pd.concat(results)
    
    # 중복된 인덱스에 대해 평균 계산
    final_result = final_result.groupby(level=0).mean()
    
    return final_result

# 실제 사용 예시
# result = process_large_csv('large_data.csv', chunk_size=50000)
``` 